#!/usr/bin/env python3
"""
TN3Kæ•°æ®é›†çš„Few-shotå­¦ä¹ è„šæœ¬
åŸºäºæ”¹è¿›çš„é‡‡æ ·ç­–ç•¥å’Œå¢å¼ºæŠ€æœ¯
"""

import os
import sys
import argparse
import random
from pathlib import Path
import numpy as np
from collections import defaultdict
import shutil
import cv2
from scipy.ndimage import distance_transform_edt
import pandas as pd
import matplotlib
matplotlib.use('Agg')  # ä½¿ç”¨éäº¤äº’å¼åç«¯
import matplotlib.pyplot as plt

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import DataLoader, Subset, Dataset
from PIL import Image
import torchvision.transforms as T
from tqdm import tqdm

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.append(str(Path(__file__).parent))

from dataset import FolderDataset
from dataset_mvtec import MVTecDataset, MVTecFewShotSampler
from dpt import DPT
from dpt_enhanced import DPTEnhanced


def set_seed(seed=42):
    """å›ºå®šéšæœºç§å­ä»¥ç¡®ä¿å¯å¤ç°æ€§"""
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False
    print(f"ğŸ² éšæœºç§å­å·²è®¾ç½®ä¸º: {seed}")


def compute_dice_coefficient(pred, target, smooth=1e-6):
    """
    è®¡ç®— Dice ç³»æ•°
    
    Args:
        pred: é¢„æµ‹ç»“æœ (torch.Tensor or np.ndarray)
        target: çœŸå®æ ‡ç­¾ (torch.Tensor or np.ndarray)
        smooth: å¹³æ»‘é¡¹ï¼Œé˜²æ­¢é™¤é›¶
    
    Returns:
        dice: Diceç³»æ•° (float)
    """
    if isinstance(pred, torch.Tensor):
        pred = pred.cpu().numpy()
    if isinstance(target, torch.Tensor):
        target = target.cpu().numpy()
    
    pred = pred.flatten()
    target = target.flatten()
    
    intersection = np.sum(pred * target)
    dice = (2. * intersection + smooth) / (np.sum(pred) + np.sum(target) + smooth)
    
    return dice


def compute_hd95(pred, target, voxel_spacing=(1.0, 1.0)):
    """
    è®¡ç®— Hausdorff Distance 95% (HD95)
    
    Args:
        pred: é¢„æµ‹ç»“æœ (np.ndarray), shape [H, W]
        target: çœŸå®æ ‡ç­¾ (np.ndarray), shape [H, W]
        voxel_spacing: åƒç´ é—´è· (tuple)
    
    Returns:
        hd95: HD95è·ç¦» (float), å¦‚æœæ— æ³•è®¡ç®—è¿”å› np.inf
    """
    if isinstance(pred, torch.Tensor):
        pred = pred.cpu().numpy()
    if isinstance(target, torch.Tensor):
        target = target.cpu().numpy()
    
    pred = pred.astype(bool)
    target = target.astype(bool)
    
    # å¦‚æœé¢„æµ‹æˆ–çœŸå®æ ‡ç­¾å…¨0æˆ–å…¨1ï¼Œæ— æ³•è®¡ç®—HD
    if pred.sum() == 0 or target.sum() == 0:
        return np.inf
    if pred.sum() == pred.size or target.sum() == target.size:
        return np.inf
    
    # è®¡ç®—è¾¹ç•Œ
    pred_border = pred ^ cv2.erode(pred.astype(np.uint8), np.ones((3,3), np.uint8), iterations=1).astype(bool)
    target_border = target ^ cv2.erode(target.astype(np.uint8), np.ones((3,3), np.uint8), iterations=1).astype(bool)
    
    if pred_border.sum() == 0 or target_border.sum() == 0:
        return np.inf
    
    # è®¡ç®—è·ç¦»å˜æ¢
    dt_pred = distance_transform_edt(~pred_border, sampling=voxel_spacing)
    dt_target = distance_transform_edt(~target_border, sampling=voxel_spacing)
    
    # è®¡ç®—ä»é¢„æµ‹è¾¹ç•Œåˆ°çœŸå®è¾¹ç•Œçš„è·
    distances_pred_to_target = dt_target[pred_border]
    # è®¡ç®—ä»çœŸå®è¾¹ç•Œåˆ°é¢„æµ‹è¾¹ç•Œçš„è·
    distances_target_to_pred = dt_pred[target_border]
    
    # åˆå¹¶æ‰€æœ‰è·
    all_distances = np.concatenate([distances_pred_to_target, distances_target_to_pred])
    
    if len(all_distances) == 0:
        return np.inf
    
    # è®¡ç®—95ç™¾åˆ†ä½æ•°
    hd95 = np.percentile(all_distances, 95)
    
    return hd95


# ===== ViSA Dataset =====
class ViSADataset(torch.utils.data.Dataset):
    """
    ViSA Dataset for Few-shot Learning
    
    Dataset structure:
    visa/
      â”œâ”€â”€ split_csv/
      â”‚  â””â”€â”€ 2cls_fewshot.csv
      â”œâ”€â”€ candle/
      â”‚  â””â”€â”€ Data/
      
      â”œâ”€â”€ Images/
      
      â”‚  â”œâ”€â”€ Normal/
      
      â”‚  â””â”€â”€ Anomaly/
      
      â””â”€â”€ Masks/
      
          â””â”€â”€ Anomaly/  (Maskå€¼ 0-6, >0å³ä¸ºå‰æ™¯)
      â””â”€â”€ capsules/
          â””â”€â”€ ...
    """
    
    def __init__(self, root, csv_file, split='train', category=None, transform=None, target_size=(512, 512)):
        """
        Args:
            root: ViSAæ•°æ®é›†æ ¹ç›®å½•
            csv_file: CSVæ–‡ä»¶è·¯å¾„ (e.g. 'split_csv/2cls_fewshot.csv')
            split: 'train' or 'test'
            category: ç‰¹å®šç±»åˆ« (e.g. 'candle'), Noneè¡¨ç¤ºæ‰€æœ‰ç±»åˆ«
            transform: æ•°æ®å¢å¼º
            target_size: ç›®æ ‡å°ºå¯¸ (H, W)
        """
        self.root = root
        self.split = split
        self.category = category
        self.transform = transform
        self.target_size = target_size
        
        # åŠ è½½CSV
        csv_path = os.path.join(root, csv_file)
        if not os.path.isfile(csv_path):
            raise FileNotFoundError(f"CSV file not found: {csv_path}")
        
        df = pd.read_csv(csv_path)
        
        # è¿‡æ»¤split
        df = df[df['split'] == split]
        
        # è¿‡æ»¤category
        if category is not None:
            df = df[df['object'] == category]
        
        # æ”¶é›†æ ·æœ¬
        self.samples = []
        for _, row in df.iterrows():
            img_path = os.path.join(root, row['image'])
            
            if not os.path.isfile(img_path):
                continue
            
            # å¤„ç†maskè·¯å¾„
            if pd.isna(row['mask']) or row['mask'] == '':
                mask_path = None  # æ­£å¸¸æ ·æœ¬æ— mask
            else:
                mask_path = os.path.join(root, row['mask'])
                if not os.path.isfile(mask_path):
                    # å°è¯•.pngæ‰©å±•
                    mask_path_png = os.path.splitext(mask_path)[0] + '.png'
                    if os.path.isfile(mask_path_png):
                        mask_path = mask_path_png
                    else:
                        continue
            
            self.samples.append({
                'image': img_path,
                'mask': mask_path,
                'label': row['label'],  # 'normal' or 'anomaly'
                'object': row['object'],
            })
        
        if len(self.samples) == 0:
            raise RuntimeError(f"No valid samples found for {category}/{split}!")
    
    def __len__(self):
        return len(self.samples)
    
    def __getitem__(self, idx):
        sample = self.samples[idx]
        
        # è¯»å–å›¾åƒ
        image = Image.open(sample['image']).convert('RGB')
        
        # è¯»å–mask
        if sample['mask'] is not None:
            mask = Image.open(sample['mask'])
            mask_np = np.array(mask)
            # ViSAçš„maskå€¼æ˜¯0-6ï¼Œ>0å³ä¸ºå‰æ™¯ (è½¬æ¢ä¸ºäºŒå€¼ 0=èƒŒæ™¯, 1=å‰æ™¯)
            mask_binary = (mask_np > 0).astype(np.uint8)
        else:
            # æ­£å¸¸æ ·æœ¬æ•° mask
            mask_binary = np.zeros(image.size[::-1], dtype=np.uint8)
        
        # Resizeåˆ°ç›®æ ‡å°º
        image = image.resize((self.target_size[1], self.target_size[0]), Image.BILINEAR)
        mask_img = Image.fromarray(mask_binary)
        mask_img = mask_img.resize((self.target_size[1], self.target_size[0]), Image.NEAREST)
        mask_binary = np.array(mask_img)
        
        # è½¬æ¢ä¸ºtensor
        if self.transform:
            image = self.transform(image)
        else:
            image = T.ToTensor()(image)
        
        mask_tensor = torch.from_numpy(mask_binary).long()
        
        # è¿”å›3ä¸ªå€¼ä»¥ä¿æŒä¸€è‡´(image, mask, label)
        # label: 0=normal, 1=anomaly
        label = 1 if sample['label'] == 'anomaly' else 0
        
        return image, mask_tensor, label


class FewShotSamplerTN3K:
    """TN3Kæ•°æ®é›†çš„Few-shoté‡‡æ ·"""
    
    def __init__(self, dataset):
        self.dataset = dataset
        self._analyze_dataset()
    
    def _analyze_dataset(self):
        """åˆ†ææ•°æ®é›†ï¼ŒæŒ‰ç…§å‰æ™¯åƒç´ æ•°æ’"""
        print("æ­£åœ¨åˆ†æTN3Kæ•°æ®é›†..")
        
        self.samples_with_target = []  # å­˜å‚¨ (idx, foreground_pixel_count)
        
        for idx in range(len(self.dataset)):
            img, mask, _ = self.dataset[idx]  # FolderDatasetè¿”å› (img_tensor, mask_tensor, _)
            
            # FolderDatasetåœ¨æ²¡æœ‰transformæ—¶ä¼šè¿”å›å·²å½’ä¸€åŒ–çš„tensor
            if torch.is_tensor(mask):
                # maskå·²ç»æ˜¯äºŒå€¼åŒ–çš„[C, H, W] æˆ– [H, W]
                if mask.dim() == 3:
                    mask = mask.squeeze(0)
                foreground_pixels = mask.sum().item()
            elif isinstance(mask, np.ndarray):
                # åŸå§‹numpyæ•°ç»„
                foreground_pixels = np.sum(mask > 127)
            else:
                foreground_pixels = 0
            
            if foreground_pixels > 0:
                self.samples_with_target.append((idx, int(foreground_pixels)))
        
        # æŒ‰å‰æ™¯åƒç´ æ•°é™åºæ’åº
        self.samples_with_target.sort(key=lambda x: x[1], reverse=True)
        
        print(f"  æ‰¾åˆ° {len(self.samples_with_target)} ä¸ªåŒ…å«ç›®æ ‡çš„æ ·æœ¬")
        if len(self.samples_with_target) > 0:
            print(f"  å‰æ™¯åƒç´ æ•°èŒƒå›´ {self.samples_with_target[-1][1]} ~ {self.samples_with_target[0][1]}")
    
    def sample_k_shot(self, k_shot, strategy='top'):
        """
        é‡‡æ ·k-shotæ ·æœ¬
        
        Args:
            k_shot: é‡‡æ ·æ•°é‡
            strategy: 'top' é€‰æ‹©å‰æ™¯æœ€å¤šçš„, 'diverse' å‡åŒ€åˆ†å¸ƒé‡‡æ ·
        """
        if strategy == 'top':
            # é€‰æ‹©å‰æ™¯åƒç´ æœ€å¤šçš„kä¸ªæ ·
            selected = [idx for idx, _ in self.samples_with_target[:k_shot]]
            print(f"\né‡‡æ ·ç­–ç•¥: é€‰æ‹©å‰æ™¯åƒç´ æœ€å¤šçš„ {k_shot} ä¸ªæ ·æœ¬")
        else:
            # å‡åŒ€åˆ†å¸ƒé‡‡æ ·
            step = len(self.samples_with_target) // k_shot
            selected = [self.samples_with_target[i * step][0] for i in range(k_shot)]
            print(f"\né‡‡æ ·ç­–ç•¥: å‡åŒ€åˆ†å¸ƒé‡‡æ · {k_shot} ä¸ªæ ·æœ¬")
        
        # æ‰“å°é€‰ä¸­æ ·æœ¬çš„ä¿¡
        print(f"é€‰ä¸­çš„æ ·æœ¬ç´¢å¼•å’Œå‰æ™¯åƒç´ æ•°")
        for idx in selected[:5]:  # åªæ˜¾ç¤ºå‰5
            pixel_count = next(cnt for i, cnt in self.samples_with_target if i == idx)
            print(f"  æ ·æœ¬ {idx}: {pixel_count} å‰æ™¯åƒç´ ")
        if len(selected) > 5:
            print(f"  ... (å…±{len(selected)} ä¸ªæ ·æœ¬")
        
        return selected


class FewShotSamplerViSA:
    """ViSAæ•°æ®é›†çš„Few-shoté‡‡æ ·"""
    
    def __init__(self, dataset):
        self.dataset = dataset
        self._analyze_dataset()
    
    def _analyze_dataset(self):
        """åˆ†ææ•°æ®é›†ï¼ŒæŒ‰ç…§å‰æ™¯åƒç´ æ•°æ’"""
        print("æ­£åœ¨åˆ†æViSAæ•°æ®é›†..")
        
        self.anomaly_samples = []  # (idx, foreground_pixel_count)
        self.normal_samples = []   # idx
        
        for idx in range(len(self.dataset)):
            sample = self.dataset.samples[idx]
            
            if sample['label'] == 'anomaly':
                # æ£€æŸ¥æ˜¯å¦çœŸçš„æœ‰å¼‚å¸¸åŒºåŸŸ
                if sample['mask'] is not None:
                    try:
                        _, mask, _ = self.dataset[idx]
                        foreground_pixels = mask.sum().item()
                        if foreground_pixels > 0:
                            self.anomaly_samples.append((idx, foreground_pixels))
                    except:
                        pass
            elif sample['label'] == 'normal':
                self.normal_samples.append(idx)
        
        # æŒ‰å‰æ™¯åƒç´ æ•°é™åºæ’åº
        self.anomaly_samples.sort(key=lambda x: x[1], reverse=True)
        
        print(f"  æ‰¾åˆ° {len(self.anomaly_samples)} ä¸ªå¼‚å¸¸æ ·æœ¬")
        print(f"  æ‰¾åˆ° {len(self.normal_samples)} ä¸ªæ­£å¸¸æ ·æœ¬")
        if self.anomaly_samples:
            print(f"  å¼‚å¸¸å‰æ™¯åƒç´ æ•°èŒƒå›´ {self.anomaly_samples[-1][1]} ~ {self.anomaly_samples[0][1]}")
    
    def sample_k_shot(self, k_shot, include_normal=False, strategy='top'):
        """
        é‡‡æ ·k-shotæ ·æœ¬
        
        Args:
            k_shot: é‡‡æ ·æ•°é‡
            include_normal: æ˜¯å¦åŒ…å«æ­£å¸¸æ ·æœ¬
            strategy: 'top' é€‰æ‹©å‰æ™¯æœ€å¤šçš„, 'diverse' å‡åŒ€åˆ†å¸ƒé‡‡æ ·
        """
        selected = []
        
        # é‡‡æ ·å¼‚å¸¸æ ·æœ¬
        if strategy == 'top':
            # é€‰æ‹©å‰æ™¯åƒç´ æœ€å¤šçš„kä¸ªå¼‚å¸¸æ ·
            anomaly_indices = [idx for idx, _ in self.anomaly_samples[:k_shot]]
            print(f"\né‡‡æ ·ç­–ç•¥: é€‰æ‹©å‰æ™¯åƒç´ æœ€å¤šçš„ {k_shot} ä¸ªå¼‚å¸¸æ ·æœ¬")
        else:
            # å‡åŒ€åˆ†å¸ƒé‡‡æ ·
            step = len(self.anomaly_samples) // k_shot
            anomaly_indices = [self.anomaly_samples[i * step][0] for i in range(k_shot)]
            print(f"\né‡‡æ ·ç­–ç•¥: å‡åŒ€åˆ†å¸ƒé‡‡æ · {k_shot} ä¸ªå¼‚å¸¸æ ·æœ¬")
        
        selected.extend(anomaly_indices)
        
        # å¯é€‰ï¼šæ·»åŠ æ­£å¸¸æ ·æœ¬
        if include_normal and self.normal_samples:
            normal_k = min(k_shot, len(self.normal_samples))
            normal_indices = random.sample(self.normal_samples, normal_k)
            selected.extend(normal_indices)
            print(f"  é¢å¤–æ·»åŠ  {normal_k} ä¸ªæ­£å¸¸æ ·æœ¬")
        
        # æ‰“å°é€‰ä¸­æ ·æœ¬ä¿¡æ¯
        print(f"é€‰ä¸­çš„å¼‚å¸¸æ ·æœ¬ç´¢å¼•å’Œå‰æ™¯åƒç´ æ•°")
        for idx in anomaly_indices[:5]:
            pixel_count = next(cnt for i, cnt in self.anomaly_samples if i == idx)
            print(f"  æ ·æœ¬ {idx}: {pixel_count} å‰æ™¯åƒç´ ")
        if len(anomaly_indices) > 5:
            print(f"  ... (å…±{len(anomaly_indices)} ä¸ªå¼‚å¸¸æ ·æœ¬")
        
        print(f"âœ… ViSAé‡‡æ ·å®Œæˆ: å…±é€‰ä¸­ {len(selected)} ä¸ªæ ·æœ¬")
        
        return selected


class TN3KTestDataset(Dataset):
    """TN3Kæµ‹è¯•é›†çš„Wrapperï¼Œå¤„ç†æ²¡æœ‰transformçš„åŸå§‹æ•°"""
    
    def __init__(self, base_dataset, target_size=(512, 512)):
        self.base_dataset = base_dataset
        self.target_size = target_size
    
    def __len__(self):
        return len(self.base_dataset)
    
    def __getitem__(self, idx):
        image, mask, label = self.base_dataset[idx]
        
        # å¤„ç†å›¾åƒ
        if not torch.is_tensor(image):
            # numpyæ•°ç»„: BGR uint8 -> RGB float tensor [C,H,W]
            image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
            image_pil = Image.fromarray(image_rgb)
            image_pil = image_pil.resize((self.target_size[1], self.target_size[0]), Image.BILINEAR)
            image = T.ToTensor()(image_pil)
            image = T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])(image)
        else:
            # å·²ç»æ˜¯å½’ä¸€åŒ–çš„tensorï¼Œä½†æ²¡æœ‰æ ‡å‡†
            if image.shape[-2:] != self.target_size:
                image = F.interpolate(image.unsqueeze(0), size=self.target_size, 
                                    mode='bilinear', align_corners=False).squeeze(0)
            image = T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])(image)
        
        # å¤„ç†mask
        if not torch.is_tensor(mask):
            # numpyæ•°ç»„
            mask_pil = Image.fromarray(mask)
            mask_pil = mask_pil.resize((self.target_size[1], self.target_size[0]), Image.NEAREST)
            mask_np = np.array(mask_pil)
            mask_binary = (mask_np > 127).astype(np.float32)
            mask = torch.from_numpy(mask_binary)
        else:
            # å·²ç»æ˜¯tensor
            if mask.dim() == 3:
                mask = mask.squeeze(0)
            if mask.shape[-2:] != self.target_size:
                mask = F.interpolate(mask.unsqueeze(0).unsqueeze(0), 
                                   size=self.target_size,
                                   mode='nearest').squeeze(0).squeeze(0)
        
        return image, mask, label


class DSIFNDataset(Dataset):
    """
    DSIFNé¥æ„Ÿå˜åŒ–æ£€æµ‹æ•°æ®é›† - ä½œä¸ºå¼‚å¸¸æ£€æµ‹æ•°æ®é›†ä½¿ç”¨
    
    æ•°æ®ç»“æ„
    train/val:
        - t1/: æ­£å¸¸å›¾åƒ (512Ã—512 JPG)
        - t2/: å¼‚å¸¸å›¾åƒ (512Ã—512 JPG) 
        - mask/: t1æ ‡ç­¾ (512Ã—512 PNG) - ä¸ä½¿
        - mask_256/: t2æ ‡ç­¾ (256Ã—256 RGB PNG) - ä½¿ç”¨æ­¤ä½œä¸ºå¼‚å¸¸æ ‡
    
    test:
        - t1/: æ­£å¸¸å›¾åƒ (512Ã—512 JPG)
        - t2/: å¼‚å¸¸å›¾åƒ (512Ã—512 JPG)
        - mask/: å…±ç”¨æ ‡ç­¾ (512Ã—512 PNG) - ä½¿ç”¨æ­¤ä½œä¸ºå¼‚å¸¸æ ‡
    
    å¼‚å¸¸æ£€æµ‹èŒƒå¼ï¼š
    - t2 ä½œä¸ºè¾“å…¥ï¼ˆå¼‚å¸¸å›¾åƒï¼‰
    - mask_256 (train/val) æˆ– mask (test) ä½œä¸ºå¼‚å¸¸åŒºåŸŸæ ‡æ³¨
    """
    
    def __init__(self, root, split='train', target_size=(512, 512), transform=None):
        """
        Args:
            root: æ•°æ®é›†æ ¹ç›®å½• (å¦‚ /home/czz/segdino/segdata/DSIFN)
            split: 'train', 'val', æˆ– 'test'
            target_size: è¾“å‡ºå›¾åƒå°ºå¯¸
            transform: å›¾åƒå˜æ¢
        """
        self.root = Path(root)
        self.split = split
        self.target_size = target_size
        self.transform = transform
        
        # æ„å»ºè·¯å¾„
        split_dir = self.root / split
        self.t1_dir = split_dir / 't1'  # æ­£å¸¸å›¾åƒï¼ˆä¸ç”¨äºè®­ç»ƒ
        self.t2_dir = split_dir / 't2'  # å¼‚å¸¸å›¾åƒï¼ˆç”¨äºè®­ç»ƒï¼‰
        
        # æ ¹æ®splité€‰æ‹©æ­£ç¡®çš„maskç›®å½•
        if split in ['train', 'val']:
            # train/valä½¿ç”¨mask_256 (t2å¯¹åº”çš„256Ã—256æ ‡ç­¾)
            self.mask_dir = split_dir / 'mask_256'
            self.is_train_val = True
        else:  # test
            # testä½¿ç”¨mask (512Ã—512æ ‡ç­¾)
            self.mask_dir = split_dir / 'mask'
            self.is_train_val = False
        
        # è·å–æ‰€æœ‰æ ·
        self.samples = []
        if self.t2_dir.exists():  # ä¸»è¦ä½¿ç”¨t2ï¼ˆå¼‚å¸¸å›¾åƒï¼‰
            for t2_path in sorted(self.t2_dir.glob('*.jpg')):
                sample_id = t2_path.stem
                mask_path = self.mask_dir / f'{sample_id}.png'
                
                if mask_path.exists():
                    self.samples.append({
                        't2_anomaly': str(t2_path),    # å¼‚å¸¸å›¾åƒï¼ˆç”¨äºè®­ç»ƒï¼‰
                        'mask': str(mask_path),        # å¼‚å¸¸æ ‡æ³¨
                        'id': sample_id,
                        'is_train_val': self.is_train_val
                    })
        
        if len(self.samples) == 0:
            raise RuntimeError(f"No samples found in {split_dir}")
        
        mask_info = "mask_256 (256Ã—256)" if self.is_train_val else "mask (512Ã—512)"
        print(f"ğŸ“Š DSIFN {split}: åŠ è½½ {len(self.samples)} ä¸ªæ ·æœ¬")
        print(f"   ğŸ“Œ ä½¿ç”¨ t2 ä½œä¸ºå¼‚å¸¸å›¾åƒ")
        print(f"   ğŸ“Œ ä½¿ç”¨ {mask_info} ä½œä¸ºå¼‚å¸¸æ ‡æ³¨")
    
    def __len__(self):
        return len(self.samples)
    
    def __getitem__(self, idx):
        sample = self.samples[idx]
        
        # è¯»å–å¼‚å¸¸å›¾åƒ (t2)
        t2_anomaly = Image.open(sample['t2_anomaly']).convert('RGB')
        
        # è¯»å–å¼‚å¸¸åŒºåŸŸæ ‡æ³¨
        mask = Image.open(sample['mask'])
        
        # å¤„ç†maskæ ¼å¼
        if mask.mode == 'RGB':
            # RGBæ ¼å¼ï¼ˆtrain/valçš„mask_256
            mask_np = np.array(mask)[:, :, 0]  # å–ç¬¬ä¸€é€šé“
        else:
            # ç°åº¦æ ¼å¼ï¼ˆtestçš„mask
            mask_np = np.array(mask)
        
        # äºŒå€¼åŒ–: 0=æ­£å¸¸, 1=å¼‚å¸¸
        mask_binary = (mask_np > 127).astype(np.uint8)
        
        # Resizeåˆ°ç›®æ ‡å°ºå¯¸(ç»Ÿä¸€512Ã—512)
        t2_anomaly = t2_anomaly.resize((self.target_size[1], self.target_size[0]), Image.BILINEAR)
        
        # maskä¹Ÿresizeåˆ°512Ã—512 (å¦‚æœæ˜¯mask_256åˆ™éœ€è¦æ”¾å¤§ï¼‰
        mask_img = Image.fromarray(mask_binary)
        mask_img = mask_img.resize((self.target_size[1], self.target_size[0]), Image.NEAREST)
        mask_binary = np.array(mask_img)
        
        # è½¬æ¢ä¸ºtensor
        if self.transform:
            t2_anomaly = self.transform(t2_anomaly)
        else:
            t2_anomaly = T.ToTensor()(t2_anomaly)
        
        mask_tensor = torch.from_numpy(mask_binary).long()
        
        # è¿”å›: (å¼‚å¸¸å›¾åƒ, å¼‚å¸¸æ ‡æ³¨, æ ‡ç­¾)
        # æ ‡ç­¾1è¡¨ç¤ºè¿™æ˜¯å¼‚å¸¸æ ·æœ¬ï¼ˆç”¨äºå¼‚å¸¸æ£€æµ‹ä»»åŠ¡ï¼‰
        return t2_anomaly, mask_tensor, 1


class FewShotSamplerDSIFN:
    """
    DSIFNæ•°æ®é›†çš„Few-shoté‡‡æ ·å™¨ï¼ˆå¼‚å¸¸æ£€æµ‹æ¨¡å¼ï¼‰
    
    é‡‡æ ·ç­–ç•¥
    - ä»å¼‚å¸¸æ ·æœ¬ï¼ˆt2å›¾åƒï¼‰ä¸­é€‰æ‹©å¼‚å¸¸åŒºåŸŸæœ€æ˜¾è‘—çš„kä¸ªæ ·
    - è¿™äº›æ ·æœ¬ç”¨äºè®­ç»ƒæ¨¡å‹è¯†åˆ«å¼‚å¸¸/å˜åŒ–æ¨¡å¼
    """
    
    def __init__(self, dataset):
        self.dataset = dataset
        self._analyze_dataset()
    
    def _analyze_dataset(self):
        """åˆ†ææ•°æ®é›†ï¼ŒæŒ‰ç…§å¼‚å¸¸åƒç´ æ•°æ’"""
        print("æ­£åœ¨åˆ†æDSIFNæ•°æ®é›†ï¼ˆå¼‚å¸¸æ£€æµ‹æ¨¡å¼ï¼‰...")
        
        self.anomaly_samples = []  # å­˜å‚¨ (idx, anomaly_pixel_count)
        
        for idx in range(len(self.dataset)):
            _, mask, _ = self.dataset[idx]
            
            if torch.is_tensor(mask):
                anomaly_pixels = mask.sum().item()
            elif isinstance(mask, np.ndarray):
                anomaly_pixels = np.sum(mask > 0)
            else:
                anomaly_pixels = 0
            
            if anomaly_pixels > 0:
                self.anomaly_samples.append((idx, int(anomaly_pixels)))
        
        # æŒ‰å¼‚å¸¸åƒç´ æ•°é™åºæ’åº
        self.anomaly_samples.sort(key=lambda x: x[1], reverse=True)
        
        print(f"  âœ… æ‰¾åˆ° {len(self.anomaly_samples)} ä¸ªåŒ…å«å¼‚å¸¸çš„æ ·æœ¬")
        if len(self.anomaly_samples) > 0:
            print(f"  ğŸ“Š å¼‚å¸¸åƒç´ æ•°èŒƒå›´ {self.anomaly_samples[-1][1]} ~ {self.anomaly_samples[0][1]}")
    
    def sample_k_shot(self, k_shot, strategy='top'):
        """
        é‡‡æ ·k-shotå¼‚å¸¸æ ·æœ¬
        
        Args:
            k_shot: é‡‡æ ·æ•°é‡
            strategy: 'top' é€‰æ‹©å¼‚å¸¸æœ€æ˜¾è‘—çš„ 'diverse' å‡åŒ€åˆ†å¸ƒé‡‡æ ·
        """
        if strategy == 'top':
            selected = [idx for idx, _ in self.anomaly_samples[:k_shot]]
            print(f"\nğŸ¯ é‡‡æ ·ç­–ç•¥: é€‰æ‹©å¼‚å¸¸åŒºåŸŸæœ€æ˜¾è‘—çš„{k_shot} ä¸ªæ ·æœ¬")
        else:
            # å‡åŒ€åˆ†å¸ƒé‡‡æ ·
            step = len(self.anomaly_samples) // k_shot
            selected = [self.anomaly_samples[i * step][0] for i in range(k_shot)]
            print(f"\nğŸ¯ é‡‡æ ·ç­–ç•¥: å‡åŒ€åˆ†å¸ƒé‡‡æ · {k_shot} ä¸ªæ ·æœ¬")
        
        # æ‰“å°é€‰ä¸­æ ·æœ¬ä¿¡æ¯
        print(f"é€‰ä¸­çš„å¼‚å¸¸æ ·æœ¬ç´¢å¼•å’Œå‰æ™¯åƒç´ æ•°")
        for idx in selected[:5]:
            pixel_count = next(cnt for i, cnt in self.anomaly_samples if i == idx)
            print(f"  æ ·æœ¬ {idx}: {pixel_count} å‰æ™¯åƒç´ ")
        if len(selected) > 5:
            print(f"  ... (å…±{len(selected)} ä¸ªæ ·æœ¬")
        
        return selected


class MassachusettsRoadsDataset(Dataset):
    """
    Massachusetts Roads é¥æ„Ÿé“è·¯åˆ†å‰²æ•°æ®
    
    æ•°æ®ç»“æ„
    - data/: åŸå§‹é¥æ„Ÿå›¾åƒ (1500Ã—1500 TIFF, RGB)
    - label/: é“è·¯æ ‡ç­¾ (1500Ã—1500 TIFF, äºŒå€¼/255)
    
    é“è·¯åˆ†å‰²ä»»åŠ¡
    - è¾“å…¥: é¥æ„Ÿå›¾åƒ
    - è¾“å‡º: é“è·¯åŒºåŸŸåˆ†å‰² (0=èƒŒæ™¯, 1=é“è·¯)
    - é“è·¯å æ¯”3-5%ï¼Œç±»åˆ«ä¸å¹³è¡¡
    """
    
    def __init__(self, root, split='train', train_ratio=0.8, target_size=(512, 512), 
                 transform=None, seed=42):
        """
        Args:
            root: æ•°æ®é›†æ ¹ç›®å½• (å¦‚ /home/czz/segdino/segdata/Massachusetts Roads)
            split: 'train' æˆ– 'test'
            train_ratio: è®­ç»ƒé›†æ¯”ä¾‹(é»˜è®¤0.8ï¼Œå³49å¼ å›¾åƒ39å¼ è®­ç»ƒï¼Œ10å¼ æµ‹è¯•)
            target_size: è¾“å‡ºå›¾åƒå°ºå¯¸
            transform: å›¾åƒå˜æ¢
            seed: éšæœºç§å­
        """
        self.root = Path(root)
        self.split = split
        self.target_size = target_size
        self.transform = transform
        
        # è·å–æ‰€æœ‰æ ·æœ¬æ–‡
        data_dir = self.root / 'data'
        label_dir = self.root / 'label'
        
        # æ”¶é›†æ‰€æœ‰é…å¯¹çš„å›¾åƒå’Œæ ‡
        all_samples = []
        for img_path in sorted(data_dir.glob('*.tiff')):
            # ä» test_data_1.tiff æå– 1
            sample_id = img_path.stem.replace('test_data_', '')
            label_path = label_dir / f'test_label_{sample_id}.tiff'
            
            if label_path.exists():
                all_samples.append({
                    'image': str(img_path),
                    'label': str(label_path),
                    'id': sample_id
                })
        
        # åˆ’åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•
        np.random.seed(seed)
        n_total = len(all_samples)
        n_train = int(n_total * train_ratio)
        
        indices = np.random.permutation(n_total)
        train_indices = indices[:n_train]
        test_indices = indices[n_train:]
        
        if split == 'train':
            self.samples = [all_samples[i] for i in train_indices]
        else:  # test
            self.samples = [all_samples[i] for i in test_indices]
        
        if len(self.samples) == 0:
            raise RuntimeError(f"No samples found in {split} split")
        
        print(f"ğŸ“Š Massachusetts Roads {split}: åŠ è½½ {len(self.samples)} ä¸ªæ ·æœ¬")
        print(f"   ğŸ“ å›¾åƒå°ºå¯¸: 1500Ã—1500 â†’ {target_size[0]}Ã—{target_size[1]}")
        print(f"   ğŸ¯ ä»»åŠ¡: é“è·¯åˆ†å‰² (äºŒåˆ†ç±»)")
    
    def __len__(self):
        return len(self.samples)
    
    def __getitem__(self, idx):
        sample = self.samples[idx]
        
        # è¯»å–TIFFå›¾åƒ
        image = Image.open(sample['image']).convert('RGB')
        label = Image.open(sample['label'])
        
        # å¤„ç†æ ‡ç­¾ (0/255 â†’ 0/1)
        label_np = np.array(label)
        label_binary = (label_np > 127).astype(np.uint8)
        
        # Resizeåˆ°ç›®æ ‡å°º
        image = image.resize((self.target_size[1], self.target_size[0]), Image.BILINEAR)
        label_img = Image.fromarray(label_binary)
        label_img = label_img.resize((self.target_size[1], self.target_size[0]), Image.NEAREST)
        label_binary = np.array(label_img)
        
        # è½¬æ¢ä¸ºtensor
        if self.transform:
            image = self.transform(image)
        else:
            image = T.ToTensor()(image)
        
        label_tensor = torch.from_numpy(label_binary).long()
        
        # è¿”å›: (å›¾åƒ, æ ‡ç­¾, ç±»åˆ«)
        # ç±»åˆ«1è¡¨ç¤ºé“è·¯åˆ†å‰²ä»»åŠ¡
        return image, label_tensor, 1


class FewShotSamplerMassRoads:
    """
    Massachusetts Roadsæ•°æ®é›†çš„Few-shoté‡‡æ ·
    
    é‡‡æ ·ç­–ç•¥
    - ä»è®­ç»ƒé›†ä¸­é€‰æ‹©é“è·¯åŒºåŸŸæœ€æ˜¾è‘—çš„kä¸ªæ ·
    - è¿™äº›æ ·æœ¬ç”¨äºè®­ç»ƒæ¨¡å‹è¯†åˆ«é“è·¯æ¨¡å¼
    """
    
    def __init__(self, dataset):
        self.dataset = dataset
        self._analyze_dataset()
    
    def _analyze_dataset(self):
        """åˆ†ææ•°æ®é›†ï¼ŒæŒ‰ç…§é“è·¯åƒç´ æ•°æ’"""
        print("æ­£åœ¨åˆ†æMassachusetts Roadsæ•°æ®é›†..")
        
        self.road_samples = []  # å­˜å‚¨ (idx, road_pixel_count)
        
        for idx in range(len(self.dataset)):
            _, label, _ = self.dataset[idx]
            
            if torch.is_tensor(label):
                road_pixels = label.sum().item()
            elif isinstance(label, np.ndarray):
                road_pixels = np.sum(label > 0)
            else:
                road_pixels = 0
            
            if road_pixels > 0:
                self.road_samples.append((idx, int(road_pixels)))
        
        # æŒ‰é“è·¯åƒç´ æ•°é™åºæ’åº
        self.road_samples.sort(key=lambda x: x[1], reverse=True)
        
        print(f"  âœ… æ‰¾åˆ° {len(self.road_samples)} ä¸ªåŒ…å«é“è·¯çš„æ ·æœ¬")
        if len(self.road_samples) > 0:
            print(f"  ğŸ“Š é“è·¯åƒç´ æ•°èŒƒå›´ {self.road_samples[-1][1]} ~ {self.road_samples[0][1]}")
    
    def sample_k_shot(self, k_shot, strategy='top'):
        """
        é‡‡æ ·k-shoté“è·¯æ ·æœ¬
        
        Args:
            k_shot: é‡‡æ ·æ•°é‡
            strategy: 'top' é€‰æ‹©é“è·¯æœ€æ˜¾è‘—çš„ 'diverse' å‡åŒ€åˆ†å¸ƒé‡‡æ ·
        """
        if strategy == 'top':
            selected = [idx for idx, _ in self.road_samples[:k_shot]]
            print(f"\nğŸ¯ é‡‡æ ·ç­–ç•¥: é€‰æ‹©é“è·¯åŒºåŸŸæœ€æ˜¾è‘—çš„{k_shot} ä¸ªæ ·æœ¬")
        else:
            # å‡åŒ€åˆ†å¸ƒé‡‡æ ·
            step = len(self.road_samples) // k_shot
            selected = [self.road_samples[i * step][0] for i in range(k_shot)]
            print(f"\nğŸ¯ é‡‡æ ·ç­–ç•¥: å‡åŒ€åˆ†å¸ƒé‡‡æ · {k_shot} ä¸ªæ ·æœ¬")
        
        # æ‰“å°é€‰ä¸­æ ·æœ¬ä¿¡æ¯
        print(f"é€‰ä¸­çš„é“è·¯æ ·æœ¬ç´¢å¼•å’Œåƒç´ æ•°")
        for idx in selected[:5]:
            pixel_count = next(cnt for i, cnt in self.road_samples if i == idx)
            print(f"  æ ·æœ¬ {idx}: {pixel_count} é“è·¯åƒç´ ")
        if len(selected) > 5:
            print(f"  ... (å…±{len(selected)} ä¸ªæ ·æœ¬")
        
        return selected


class SatelliteDataset(Dataset):
    """
    Satellite Dataset é¥æ„Ÿå›¾åƒåˆ†å‰²æ•°æ®
    
    æ•°æ®ç»“æ„
    - image/: é¥æ„Ÿå›¾åƒ (512Ã—512 TIFF, RGB)
    - label/: åˆ†å‰²æ ‡ç­¾ (512Ã—512 TIFF, RGBäºŒå€¼/255)
    
    é¥æ„Ÿåˆ†å‰²ä»»åŠ¡
    - è¾“å…¥: é¥æ„Ÿå›¾åƒ
    - è¾“å‡º: åŒºåŸŸåˆ†å‰² (0=èƒŒæ™¯, 1=å‰æ™¯)
    """
    
    def __init__(self, root, split='train', train_ratio=0.8, target_size=(512, 512), 
                 transform=None, seed=42):
        """
        Args:
            root: æ•°æ®é›†æ ¹ç›®å½• (å¦‚ /home/czz/segdino/segdata/Satellite dataset)
            split: 'train' æˆ– 'test'
            train_ratio: è®­ç»ƒé›†æ¯”ä¾‹(é»˜è®¤0.8)
            target_size: è¾“å‡ºå›¾åƒå°ºå¯¸
            transform: å›¾åƒå˜æ¢
            seed: éšæœºç§å­
        """
        self.root = Path(root)
        self.split = split
        self.target_size = target_size
        self.transform = transform
        
        # è·å–æ‰€æœ‰æ ·æœ¬æ–‡
        image_dir = self.root / 'image'
        label_dir = self.root / 'label'
        
        # æ”¶é›†æ‰€æœ‰é…å¯¹çš„å›¾åƒå’Œæ ‡
        all_samples = []
        for img_path in sorted(image_dir.glob('*.tif')):
            sample_id = img_path.stem
            label_path = label_dir / f'{sample_id}.tif'
            
            if label_path.exists():
                all_samples.append({
                    'image': str(img_path),
                    'label': str(label_path),
                    'id': sample_id
                })
        
        # åˆ’åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•
        np.random.seed(seed)
        n_total = len(all_samples)
        n_train = int(n_total * train_ratio)
        
        indices = np.random.permutation(n_total)
        train_indices = indices[:n_train]
        test_indices = indices[n_train:]
        
        if split == 'train':
            self.samples = [all_samples[i] for i in train_indices]
        else:  # test
            self.samples = [all_samples[i] for i in test_indices]
        
        if len(self.samples) == 0:
            raise RuntimeError(f"No samples found in {split} split")
        
        print(f"ğŸ“Š Satellite Dataset {split}: åŠ è½½ {len(self.samples)} ä¸ªæ ·æœ¬")
        print(f"   ğŸ“ å›¾åƒå°ºå¯¸: 512Ã—512")
        print(f"   ğŸ¯ ä»»åŠ¡: é¥æ„Ÿåˆ†å‰² (äºŒåˆ†ç±»)")
    
    def __len__(self):
        return len(self.samples)
    
    def __getitem__(self, idx):
        sample = self.samples[idx]
        
        # è¯»å–TIFFå›¾åƒ
        image = Image.open(sample['image']).convert('RGB')
        label = Image.open(sample['label'])
        
        # å¤„ç†RGBæ ¼å¼çš„label (å–ç¬¬ä¸€é€šé“å¹¶äºŒå€¼åŒ–)
        label_np = np.array(label)
        if len(label_np.shape) == 3:
            label_np = label_np[:, :, 0]  # å–ç¬¬ä¸€é€šé“
        label_binary = (label_np > 127).astype(np.uint8)
        
        # Resizeåˆ°ç›®æ ‡å°ºå¯¸ï¼ˆå¦‚æœéœ€è¦ï¼‰
        if image.size != (self.target_size[1], self.target_size[0]):
            image = image.resize((self.target_size[1], self.target_size[0]), Image.BILINEAR)
        
        if label_binary.shape != self.target_size:
            label_img = Image.fromarray(label_binary)
            label_img = label_img.resize((self.target_size[1], self.target_size[0]), Image.NEAREST)
            label_binary = np.array(label_img)
        
        # è½¬æ¢ä¸ºtensor
        if self.transform:
            image = self.transform(image)
        else:
            image = T.ToTensor()(image)
        
        label_tensor = torch.from_numpy(label_binary).long()
        
        # è¿”å›: (å›¾åƒ, æ ‡ç­¾, å›¾åƒè·¯å¾„)
        return image, label_tensor, sample['image']


class FewShotSamplerSatellite:
    """
    Satellite Datasetçš„Few-shoté‡‡æ ·
    
    é‡‡æ ·ç­–ç•¥
    - ä»è®­ç»ƒé›†ä¸­é€‰æ‹©å‰æ™¯åŒºåŸŸæœ€æ˜¾è‘—çš„kä¸ªæ ·
    """
    
    def __init__(self, dataset):
        self.dataset = dataset
        self._analyze_dataset()
    
    def _analyze_dataset(self):
        """åˆ†ææ•°æ®é›†ï¼ŒæŒ‰ç…§å‰æ™¯åƒç´ æ•°æ’"""
        print("æ­£åœ¨åˆ†æSatellite Dataset...")
        
        self.fg_samples = []  # å­˜å‚¨ (idx, fg_pixel_count)
        
        for idx in range(len(self.dataset)):
            _, label, _ = self.dataset[idx]
            
            if torch.is_tensor(label):
                fg_pixels = label.sum().item()
            elif isinstance(label, np.ndarray):
                fg_pixels = np.sum(label > 0)
            else:
                fg_pixels = 0
            
            if fg_pixels > 0:
                self.fg_samples.append((idx, int(fg_pixels)))
        
        # æŒ‰å‰æ™¯åƒç´ æ•°é™åºæ’åº
        self.fg_samples.sort(key=lambda x: x[1], reverse=True)
        
        print(f"  âœ… æ‰¾åˆ° {len(self.fg_samples)} ä¸ªåŒ…å«å‰æ™¯çš„æ ·æœ¬")
        if len(self.fg_samples) > 0:
            print(f"  ğŸ“Š å‰æ™¯åƒç´ æ•°èŒƒå›´ {self.fg_samples[-1][1]} ~ {self.fg_samples[0][1]}")
    
    def sample_k_shot(self, k_shot, strategy='top'):
        """
        é‡‡æ ·k-shotæ ·æœ¬
        
        Args:
            k_shot: é‡‡æ ·æ•°é‡
            strategy: 'top' é€‰æ‹©å‰æ™¯æœ€æ˜¾è‘—çš„ 'diverse' å‡åŒ€åˆ†å¸ƒé‡‡æ ·
        """
        if strategy == 'top':
            selected = [idx for idx, _ in self.fg_samples[:k_shot]]
            print(f"\nğŸ¯ é‡‡æ ·ç­–ç•¥: é€‰æ‹©å‰æ™¯åŒºåŸŸæœ€æ˜¾è‘—çš„{k_shot} ä¸ªæ ·æœ¬")
        else:
            # å‡åŒ€åˆ†å¸ƒé‡‡æ ·
            step = len(self.fg_samples) // k_shot
            selected = [self.fg_samples[i * step][0] for i in range(k_shot)]
            print(f"\nğŸ¯ é‡‡æ ·ç­–ç•¥: å‡åŒ€åˆ†å¸ƒé‡‡æ · {k_shot} ä¸ªæ ·æœ¬")
        
        # æ‰“å°é€‰ä¸­æ ·æœ¬ä¿¡æ¯
        print(f"é€‰ä¸­çš„æ ·æœ¬ç´¢å¼•å’Œåƒç´ æ•°")
        for idx in selected[:5]:
            pixel_count = next(cnt for i, cnt in self.fg_samples if i == idx)
            print(f"  æ ·æœ¬ {idx}: {pixel_count} å‰æ™¯åƒç´ ")
        if len(selected) > 5:
            print(f"  ... (å…±{len(selected)} ä¸ªæ ·æœ¬")
        
        return selected


class EnhancedFewShotDatasetTN3K(Dataset):
    """TN3Kçš„å¢å¼ºå‹Few-shotæ•°æ®"""
    
    def __init__(self, base_dataset, selected_indices, augment_factor=10, target_size=(512, 512)):
        self.base_dataset = base_dataset
        self.selected_indices = selected_indices
        self.augment_factor = augment_factor
        self.target_size = target_size
        
        # å›¾åƒå½’ä¸€åŒ–å‚æ•°(ImageNet)
        self.mean = torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1)
        self.std = torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1)
        
        # å›¾åƒæ•°æ®å¢å¼ºï¼ˆåŒ…å«é¢œè‰²å˜æ¢ï¼‰
        self.image_augmentation = T.Compose([
            T.RandomHorizontalFlip(p=0.5),
            T.RandomVerticalFlip(p=0.5),
            T.RandomRotation(degrees=15),
            T.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),
        ])
        
        # Maskæ•°æ®å¢å¼ºï¼ˆåªåŒ…å«å‡ ä½•å˜æ¢ï¼Œä¸æ”¹å˜åƒç´ å€¼ï¼‰
        self.mask_augmentation = T.Compose([
            T.RandomHorizontalFlip(p=0.5),
            T.RandomVerticalFlip(p=0.5),
            T.RandomRotation(degrees=15),
        ])
    
    def __len__(self):
        return len(self.selected_indices) * self.augment_factor
    
    def __getitem__(self, idx):
        # ç¡®å®šåŸºç¡€æ ·æœ¬ç´¢å¼•
        base_idx = self.selected_indices[idx // self.augment_factor]
        aug_idx = idx % self.augment_factor
        
        # è·å–åŸå§‹æ•°æ®
        image, mask, _ = self.base_dataset[base_idx]  # FolderDatasetè¿”å› (img_tensor, mask_tensor, _)
        
        # å¤„ç†å›¾åƒ
        if not torch.is_tensor(image):
            # numpyæ•°ç»„: BGR uint8 -> RGB float tensor [C,H,W]
            image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
            image_pil = Image.fromarray(image_rgb)
            image_pil = image_pil.resize((self.target_size[1], self.target_size[0]), Image.BILINEAR)
            image = T.ToTensor()(image_pil)
            image = T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])(image)
        else:
            # å·²ç»æ˜¯å½’ä¸€åŒ–çš„tensor [C, H, W]ï¼Œä½†æ²¡æœ‰æ ‡å‡†
            if image.shape[-2:] != self.target_size:
                image = F.interpolate(image.unsqueeze(0), size=self.target_size, 
                                    mode='bilinear', align_corners=False).squeeze(0)
            # åº”ç”¨ImageNetæ ‡å‡†
            image = T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])(image)
        
        # å¤„ç†mask
        if not torch.is_tensor(mask):
            # numpyæ•°ç»„: grayscale uint8 -> binary float tensor [H,W]
            mask_pil = Image.fromarray(mask)
            mask_pil = mask_pil.resize((self.target_size[1], self.target_size[0]), Image.NEAREST)
            mask_np = np.array(mask_pil)
            # è‡ªåŠ¨æ£€æµ‹maskå€¼èŒƒå›´æ¥äºŒå€¼åŒ–
            if mask_np.max() > 1:
                # 0-255èŒƒå›´ï¼Œä½¿ç”¨27é˜ˆ
                mask_binary = (mask_np > 127).astype(np.float32)
            else:
                # 0-1èŒƒå›´ï¼ˆå·²ç»æ˜¯äºŒå€¼ï¼‰ï¼Œç›´æ¥ä½¿
                mask_binary = mask_np.astype(np.float32)
            mask = torch.from_numpy(mask_binary)
        else:
            # å·²ç»æ˜¯äºŒå€¼åŒ–çš„tensor [C, H, W] æˆ– [H, W]
            if mask.dim() == 3:
                mask = mask.squeeze(0)  # [1, H, W] -> [H, W]
            if mask.shape[-2:] != self.target_size:
                mask = F.interpolate(mask.unsqueeze(0).unsqueeze(0), 
                                   size=self.target_size,
                                   mode='nearest').squeeze(0).squeeze(0)
            # ç¡®ä¿maskæ˜¯floatç±»å‹ï¼Œå¹¶ä¸”æ˜¯äºŒå€¼çš„0/1
            mask = mask.float()
            # å¦‚æœmaskå€¼ä¸æ˜¯/1ï¼Œéœ€è¦äºŒå€¼åŒ–
            if mask.max() > 1:
                mask = (mask > 0.5).float()
        
        # æ•°æ®å¢å¼º
        if aug_idx > 0:  # ç¬¬ä¸€ä¸ªä¸å¢å¼ºï¼Œä½œä¸ºåŸå§‹æ ·
            seed = random.randint(0, 2**32 - 1)
            
            # å›¾åƒå¢å¼ºï¼ˆåŒ…å«é¢œè‰²å˜æ¢ï¼‰
            random.seed(seed)
            torch.manual_seed(seed)
            image = self.image_augmentation(image)
            
            # maskå¢å¼ºï¼ˆåªåŒ…å«å‡ ä½•å˜æ¢
            random.seed(seed)
            torch.manual_seed(seed)
            mask_3d = mask.unsqueeze(0)  # [H, W] -> [1, H, W]
            mask_3d = self.mask_augmentation(mask_3d)
            mask = mask_3d.squeeze(0)  # [1, H, W] -> [H, W]
            
            # ç¡®ä¿maskä»ç„¶æ˜¯äºŒå€¼çš„ï¼ˆæ—‹è½¬å¯èƒ½äº§ç”Ÿæ’å€¼ï¼‰
            mask = (mask > 0.5).float()
        
        # è¿”å›3ä¸ªå€¼ä¿æŒä¸€è‡´æ€§ (image, mask, label)
        # labelè®¾ä¸º1è¡¨ç¤ºè¿™æ˜¯ä¸€ä¸ªæœ‰æ•ˆçš„è®­ç»ƒæ ·æœ¬
        return image, mask, 1


def visualize_specific_satellite_image(model, dataset, target_name='3_1.tif', save_path='vis_3_1.png', device='cuda'):
    """
    å¯è§†åŒ–ç‰¹å®šçš„Satelliteå›¾åƒ (å¦‚ 3_1.tif)
    """
    print(f"æ­£åœ¨å¯»æ‰¾å¹¶å¯è§†åŒ–ç‰¹å®šå›¾åƒ: {target_name} ...")
    target_idx = -1
    
    # æŸ¥æ‰¾å›¾åƒ
    if hasattr(dataset, 'samples'):
        for i, sample in enumerate(dataset.samples):
            # ä½¿ç”¨ os.path.basename ç¡®ä¿ç²¾ç¡®åŒ¹é…æ–‡ä»¶
            if os.path.basename(sample['image']) == target_name:
                target_idx = i
                break
    
    if target_idx == -1:
        print(f"  âš ï¸ æœªåœ¨å½“å‰æ•°æ®é›†ä¸­æ‰¾åˆ° {target_name}")
        return False
        
    print(f"  âœ… æ‰¾åˆ° {target_name} (Index: {target_idx})")
    
    # è·å–æ•°æ®
    # SatelliteDataset returns (image, label, path)
    image, mask, path = dataset[target_idx]
    
    # é¢„æµ‹
    model.eval()
    with torch.no_grad():
        image_input = image.unsqueeze(0).to(device)
        result = model(image_input)
        if isinstance(result, tuple):
            output = result[0]
        else:
            output = result
        
        pred_prob = torch.sigmoid(output).squeeze().cpu().numpy()
        pred_mask = (pred_prob > 0.6).astype(np.uint8) # Satellite optimal threshold 0.6
    
    # å‡†å¤‡æ˜¾ç¤º
    gt_mask = mask.cpu().numpy().astype(np.uint8)
    
    # åå½’ä¸€
    image_np = image.cpu().numpy().transpose(1, 2, 0)
    mean = np.array([0.485, 0.456, 0.406])
    std = np.array([0.229, 0.224, 0.225])
    image_np = image_np * std + mean
    image_np = np.clip(image_np, 0, 1)
    
    # Overlay calculation
    overlay = image_np.copy()
    tp_mask = np.logical_and(pred_mask == 1, gt_mask == 1)
    fp_mask = np.logical_and(pred_mask == 1, gt_mask == 0)
    fn_mask = np.logical_and(pred_mask == 0, gt_mask == 1)
    
    # Green for TP, Red for FP, Blue for FN
    overlay[tp_mask] = 0.6 * overlay[tp_mask] + 0.4 * np.array([0, 1, 0])
    overlay[fp_mask] = 0.6 * overlay[fp_mask] + 0.4 * np.array([1, 0, 0])
    overlay[fn_mask] = 0.6 * overlay[fn_mask] + 0.4 * np.array([0, 0, 1])

    # ç»˜å›¾
    fig, axes = plt.subplots(1, 5, figsize=(25, 5))
    
    axes[0].imshow(image_np)
    axes[0].set_title('Original Image')
    axes[0].axis('off')
    
    axes[1].imshow(gt_mask, cmap='gray')
    axes[1].set_title('Ground Truth')
    axes[1].axis('off')
    
    axes[2].imshow(pred_mask, cmap='gray')
    axes[2].set_title('Prediction')
    axes[2].axis('off')
    
    axes[3].imshow(pred_prob, cmap='jet')
    axes[3].set_title('Probability')
    axes[3].axis('off')

    axes[4].imshow(overlay)
    axes[4].set_title('Overlay\nGreen=TP, Red=FP, Blue=FN')
    axes[4].axis('off')
    
    plt.suptitle(f'Visualization of {target_name}')
    plt.tight_layout()
    plt.savefig(save_path)
    plt.close()
    print(f"  âœ… å¯è§†åŒ–ç»“æœå·²ä¿å­˜: {save_path}")
    return True


def train_few_shot_tn3k(
    data_dir='./data/TN3K',
    output_dir='./tn3k_few_shot_results',
    k_shot=5,
    epochs=50,
    batch_size=4,
    lr=1e-4,
    augment_factor=10,
    sampling_strategy='top',
    use_internal_adapter=False,  # æ˜¯å¦åœ¨DINOv3å†…éƒ¨æ³¨å…¥é€‚é…å™¨
    use_glcm=False,  # æ˜¯å¦ä½¿ç”¨GLCMæ¨¡å—
    use_hypergraph=False,
    use_layers='all',
    device='cuda',
    early_stopping_patience=10,
    seed=42,  # æ·»åŠ éšæœºç§å­å‚æ•°
    dataset_type='tn3k',  # æ–°å¢: æ•°æ®é›†ç±»
    mvtec_category=None,  # æ–°å¢: MVTecç±»åˆ«
    visa_category=None,  # æ–°å¢: ViSAç±»åˆ«
    visa_csv='split_csv/2cls_fewshot.csv',  # ViSAçš„CSVæ–‡ä»¶
    include_normal=False,  # ViSAæ˜¯å¦åŒ…å«æ­£å¸¸æ ·æœ¬
    num_classes=5,  # ç±»åˆ«æ•°é‡ (TN3K=5, ViSA=2, MVTec=2)
    val_interval=10  # éªŒè¯é—´éš”
):
    """
    Few-shotè®­ç»ƒå‡½æ•°ï¼ˆæ”¯æŒTN3Kã€MVTecå’ŒViSAæ•°æ®é›†ï¼‰
    
    Args:
        seed: éšæœºç§å­,é»˜è®¤42
        use_internal_adapter: æ˜¯å¦åœ¨DINOv3å†…éƒ¨æ³¨å…¥é€‚é…å™¨
        use_glcm: æ˜¯å¦ä½¿ç”¨GLCMå…¨å±€-å±€éƒ¨æ ¡å‡†æ¨¡å—
        dataset_type: æ•°æ®é›†ç±»å‹'tn3k', 'mvtec' æˆ– 'visa'
        mvtec_category: MVTecç±»åˆ«åç§°ï¼Œå¦‚ 'bottle'
        visa_category: ViSAç±»åˆ«åç§°ï¼Œå¦‚ 'candle'
        visa_csv: ViSAçš„CSVæ–‡ä»¶è·¯å¾„
        include_normal: ViSAæ˜¯å¦åŒ…å«æ­£å¸¸æ ·æœ¬
        num_classes: ç±»åˆ«æ•°é‡ (TN3K=5, ViSA/MVTec=2)
        val_interval: éªŒè¯é—´éš”ï¼ˆæ¯Nä¸ªepochéªŒè¯ä¸€æ¬¡ï¼‰
    """
    # è®¾ç½®éšæœºç§å­
    set_seed(seed)
    
    os.makedirs(output_dir, exist_ok=True)
    
    # æ ¹æ®æ•°æ®é›†ç±»å‹åŠ è½½æ•°
    if dataset_type == 'tn3k':
        print(f"\nåŠ è½½TN3Kæ•°æ®é›† {data_dir}")
        
        # TN3Kä¸ä½¿ç”¨ResizeAndNormalizeï¼Œå› ä¸ºå®ƒä¼šç”¨thr=0.5äºŒå€¼åŒ–
        # ç›´æ¥ä½¿ç”¨FolderDatasetï¼Œåœ¨æ•°æ®å¢å¼ºé˜¶æ®µå¤„ç†resize
        train_dataset = FolderDataset(
            root=data_dir,
            split='train',
            img_dir_name='image',
            label_dir_name='mask',
            transform=None  # ä¸ä½¿ç”¨transformï¼Œæ‰‹åŠ¨å¤„
        )
        test_dataset = FolderDataset(
            root=data_dir,
            split='test',
            img_dir_name='image',
            label_dir_name='mask',
            transform=None  # ä¸ä½¿ç”¨transformï¼Œæ‰‹åŠ¨å¤„
        )
        
        print(f"è®­ç»ƒé›†æ ·æœ¬æ•°: {len(train_dataset)}")
        print(f"æµ‹è¯•é›†æ ·æœ¬æ•°: {len(test_dataset)}")
        
        # é‡‡æ ·few-shotæ ·æœ¬
        sampler = FewShotSamplerTN3K(train_dataset)
        selected_indices = sampler.sample_k_shot(k_shot, strategy=sampling_strategy)
        
    elif dataset_type == 'visa':
        print(f"\nåŠ è½½ViSAæ•°æ®é›† {data_dir}")
        
        if visa_category is None:
            raise ValueError("ä½¿ç”¨ViSAæ•°æ®é›†æ—¶å¿…é¡»æŒ‡å®š --visa_category")
        
        print(f"ç±»åˆ«: {visa_category}")
        print(f"CSVæ–‡ä»¶: {visa_csv}")
        
        # åŠ è½½è®­ç»ƒé›†å’Œæµ‹è¯•
        train_dataset = ViSADataset(
            root=data_dir,
            csv_file=visa_csv,
            split='train',
            category=visa_category,
            target_size=(512, 512)
        )
        
        test_dataset = ViSADataset(
            root=data_dir,
            csv_file=visa_csv,
            split='test',
            category=visa_category,
            target_size=(512, 512)
        )
        
        print(f"è®­ç»ƒé›†æ ·æœ¬æ•°: {len(train_dataset)}")
        print(f"æµ‹è¯•é›†æ ·æœ¬æ•°: {len(test_dataset)}")
        
        # é‡‡æ ·few-shotæ ·æœ¬
        sampler = FewShotSamplerViSA(train_dataset)
        selected_indices = sampler.sample_k_shot(k_shot, include_normal=include_normal, strategy=sampling_strategy)
        
    elif dataset_type == 'mvtec':
        print(f"\nåŠ è½½MVTecæ•°æ®é›† {data_dir}")
        
        # æ”¯æŒåŠ è½½å•ä¸ªç±»åˆ«æˆ–æ‰€æœ‰ç±»
        if mvtec_category is None or mvtec_category == 'all':
            # åŠ è½½æ‰€æœ‰ 5ä¸ªç±»
            categories = [
                'bottle', 'cable', 'capsule', 'carpet', 'grid',
                'hazelnut', 'leather', 'metal_nut', 'pill', 'screw',
                'tile', 'toothbrush', 'transistor', 'wood', 'zipper'
            ]
            print(f"åŠ è½½æ‰€æœ‰ 5ä¸ªç‰©ä½“ç±»å‹ {', '.join(categories)}")
        else:
            # åŠ è½½æŒ‡å®šç±»åˆ«
            categories = [mvtec_category]
            print(f"åŠ è½½ç±»åˆ«: {mvtec_category}")
        
        # MVTecåªæœ‰testé›†ï¼Œæˆ‘ä»¬å°†å…¶åˆ†ä¸ºè®­ç»ƒå’Œæµ‹
        full_dataset = MVTecDataset(
            root=data_dir,
            categories=categories,
            split='test',
            target_size=(512, 512)
        )
        
        # é‡‡æ ·few-shotæ ·æœ¬ï¼ˆä»æœ‰ç¼ºé™·çš„æ ·æœ¬ä¸­é€‰æ‹©
        sampler = MVTecFewShotSampler(full_dataset)
        selected_indices = sampler.sample_k_shot(k_shot, strategy=sampling_strategy)
        
        # ä½¿ç”¨å‰©ä½™æ ·æœ¬ä½œä¸ºæµ‹è¯•
        all_indices = set(range(len(full_dataset)))
        test_indices = list(all_indices - set(selected_indices))
        
        train_dataset = full_dataset
        test_dataset = Subset(full_dataset, test_indices)
        
        print(f"è®­ç»ƒæ ·æœ¬æ•°(few-shot): {len(selected_indices)}")
        print(f"æµ‹è¯•æ ·æœ¬æ•° {len(test_dataset)}")
        
    elif dataset_type == 'dsifn':
        print(f"\nğŸŒ åŠ è½½DSIFNé¥æ„Ÿæ•°æ®é›†ï¼ˆå¼‚å¸¸æ£€æµ‹æ¨¡å¼ï¼‰: {data_dir}")
        print(f"   ğŸ“ æ•°æ®ç»“æ„:")
        print(f"      train/val: t1(æ­£å¸¸) + t2(å¼‚å¸¸) + mask_256(t2çš„256Ã—256æ ‡æ³¨)")
        print(f"      test:      t1(æ­£å¸¸) + t2(å¼‚å¸¸) + mask(512Ã—512æ ‡æ³¨)")
        print(f"   ğŸ¯ ä½¿ç”¨ t2 ä½œä¸ºå¼‚å¸¸å›¾åƒè¾“å…¥\n")
        
        # åŠ è½½è®­ç»ƒé›†å’ŒéªŒè¯
        train_dataset = DSIFNDataset(
            root=data_dir,
            split='train',
            target_size=(512, 512)
        )
        
        # ä½¿ç”¨valä½œä¸ºæµ‹è¯•
        test_dataset = DSIFNDataset(
            root=data_dir,
            split='val',
            target_size=(512, 512)
        )
        
        print(f"è®­ç»ƒé›†æ ·æœ¬æ•°: {len(train_dataset)}")
        print(f"éªŒè¯é›†æ ·æœ¬æ•°: {len(test_dataset)}")
        
        # é‡‡æ ·few-shotå¼‚å¸¸æ ·æœ¬
        sampler = FewShotSamplerDSIFN(train_dataset)
        selected_indices = sampler.sample_k_shot(k_shot, strategy=sampling_strategy)
        
    elif dataset_type == 'massroads':
        print(f"\nğŸ›£ï¸ åŠ è½½Massachusetts Roadsé¥æ„Ÿé“è·¯åˆ†å‰²æ•°æ®é›† {data_dir}")
        print(f"   ğŸ“ æ•°æ®ç»“æ„:")
        print(f"      data/: é¥æ„Ÿå›¾åƒ (1500Ã—1500 TIFF)")
        print(f"      label/: é“è·¯æ ‡ç­¾ (1500Ã—1500 TIFF, äºŒå€¼/255)")
        print(f"   ğŸ¯ ä»»åŠ¡: é“è·¯åŒºåŸŸåˆ†å‰²\n")
        
        # åŠ è½½è®­ç»ƒé›†å’Œæµ‹è¯•é›†(80%è®­ç»ƒé›†0%æµ‹è¯•)
        train_dataset = MassachusettsRoadsDataset(
            root=data_dir,
            split='train',
            train_ratio=0.8,
            target_size=(512, 512),
            seed=seed
        )
        
        test_dataset = MassachusettsRoadsDataset(
            root=data_dir,
            split='test',
            train_ratio=0.8,
            target_size=(512, 512),
            seed=seed
        )
        
        print(f"è®­ç»ƒé›†æ ·æœ¬æ•°: {len(train_dataset)}")
        print(f"æµ‹è¯•é›†æ ·æœ¬æ•°: {len(test_dataset)}")
        
        # é‡‡æ ·few-shoté“è·¯æ ·æœ¬
        sampler = FewShotSamplerMassRoads(train_dataset)
        selected_indices = sampler.sample_k_shot(k_shot, strategy=sampling_strategy)
        
    elif dataset_type == 'satellite':
        print(f"\nğŸ›°ï¸ åŠ è½½Satellite Dataseté¥æ„Ÿåˆ†å‰²æ•°æ®é›† {data_dir}")
        print(f"   ğŸ“ æ•°æ®ç»“æ„:")
        print(f"      image/: é¥æ„Ÿå›¾åƒ (512Ã—512 TIFF)")
        print(f"      label/: åˆ†å‰²æ ‡ç­¾ (512Ã—512 TIFF, RGBäºŒå€¼/255)")
        print(f"   ğŸ¯ ä»»åŠ¡: é¥æ„ŸåŒºåŸŸåˆ†å‰²\n")
        
        # åŠ è½½è®­ç»ƒé›†å’Œæµ‹è¯•é›†(80%è®­ç»ƒé›†0%æµ‹è¯•)
        train_dataset = SatelliteDataset(
            root=data_dir,
            split='train',
            train_ratio=0.8,
            target_size=(512, 512),
            seed=seed
        )
        
        test_dataset = SatelliteDataset(
            root=data_dir,
            split='test',
            train_ratio=0.8,
            target_size=(512, 512),
            seed=seed
        )
        
        print(f"è®­ç»ƒé›†æ ·æœ¬æ•°: {len(train_dataset)}")
        print(f"æµ‹è¯•é›†æ ·æœ¬æ•°: {len(test_dataset)}")
        
        # é‡‡æ ·few-shotæ ·æœ¬
        sampler = FewShotSamplerSatellite(train_dataset)
        selected_indices = sampler.sample_k_shot(k_shot, strategy=sampling_strategy)
        
    else:
        raise ValueError(f"æœªçŸ¥çš„æ•°æ®é›†ç±»å‹: {dataset_type}")
    
    # é‡‡æ ·few-shotæ ·æœ¬å·²åœ¨ä¸Šé¢å®Œæˆ
    # selected_indices å·²ç»è·å¾—
    
    # åˆ›å»ºå¢å¼ºæ•°æ®
    enhanced_dataset = EnhancedFewShotDatasetTN3K(
        base_dataset=train_dataset,
        selected_indices=selected_indices,
        augment_factor=augment_factor,
        target_size=(512, 512)
    )
    
    print(f"\nå¢å¼ºåè®­ç»ƒé›†å¤§å°: {len(enhanced_dataset)} (åŸå§‹ {len(selected_indices)} Ã— {augment_factor})")
    
    # ä¸ºæ‰€æœ‰æ•°æ®é›†åˆ›å»ºæµ‹è¯•é›†wrapper (ç¡®ä¿éªŒè¯æ—¶è¿›è¡Œæ ‡å‡†åŒ–)
    # TN3KTestDataset ä¼šå¯¹å›¾åƒè¿›è¡ŒImageNetæ ‡å‡†åŒ–ï¼Œè¿™å¯¹DINOv3æ˜¯å¿…é¡»çš„
    test_dataset_wrapped = TN3KTestDataset(test_dataset, target_size=(512, 512))
    
    # åˆ›å»ºæ•°æ®åŠ è½½
    train_loader = DataLoader(
        enhanced_dataset,
        batch_size=batch_size,
        shuffle=True,
        num_workers=4,
        pin_memory=True
    )
    
    test_loader = DataLoader(
        test_dataset_wrapped,
        batch_size=8,  # å¢å¤§batch sizeé¿å…è¶…å›¾æ¨¡å—çš„ç»´åº¦é—®
        shuffle=False,
        num_workers=4,
        pin_memory=True
    )
    
    # åˆ›å»ºæ¨¡å‹
    print("\nåˆ›å»ºDPTæ¨¡å‹...")
    repo_dir = './dinov3'
    dino_ckpt = './web_pth/dinov3_vits16_pretrain_lvd1689m-08c60483.pth'
    backbone = torch.hub.load(repo_dir, 'dinov3_vits16', source='local', weights=dino_ckpt)
    
    # æ‰€æœ‰æ•°æ®é›†éƒ½ä½¿ç”¨äºŒåˆ†ç±»ï¼ˆèƒŒæ™¯vs å‰æ™¯
    model_nclass = 1  # äºŒåˆ†ç±»ï¼ˆä½¿ç”¨BCEWithLogitsLoss
    
    print(f"  æ¨¡å‹è¾“å‡ºç±»åˆ«æ•° {model_nclass} (äºŒåˆ†ç±» èƒŒæ™¯ vs å‰æ™¯)")
    
    # åˆ¤æ–­ä½¿ç”¨å“ªç§æ¨¡å‹é…ç½®
    if use_internal_adapter or use_glcm or use_hypergraph:
        # ä½¿ç”¨å¢å¼ºç‰ˆDPT
        modules_enabled = []
        if use_internal_adapter:
            modules_enabled.append("Internal Adapter")
        if use_glcm:
            modules_enabled.append("GLCM")
        if use_hypergraph:
            modules_enabled.append("è¶…å›¾GCN")
        
        print(f"  ä½¿ç”¨å¢å¼ºç‰ˆDPTï¼ˆ{' + '.join(modules_enabled)}")
        
        model = DPTEnhanced(
            encoder_size='small',
            nclass=model_nclass,
            features=256,
            out_channels=[96, 192, 384, 768],
            use_bn=False,
            backbone=backbone,
            use_layers=use_layers,
            use_internal_adapter=use_internal_adapter,  # å†…éƒ¨é€‚é…å™¨
            use_glcm=use_glcm,  # æ ¹æ®å‚æ•°å¯ç”¨GLCM
            use_hypergraph=use_hypergraph,  # æ ¹æ®å‚æ•°å¯ç”¨è¶…å›¾GCN
            fusion_strategy='sequential'  # Internal Adapter â†’ GLCM â†’ è¶…å›¾GCN é¡ºåºå¤„ç†
        ).to(device)
    else:
        print("  ä½¿ç”¨åŸºç¡€ç‰ˆDPT")
        model = DPT(
            encoder_size='small',
            nclass=model_nclass,
            features=256,
            out_channels=[96, 192, 384, 768],
            use_bn=False,
            backbone=backbone,
            use_layers=use_layers
        ).to(device)
    
    # ä¼˜åŒ–å™¨å’Œå­¦ä¹ ç‡è°ƒ
    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=1e-4)
    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs)
    
    # æŸå¤±å‡½æ•° - æ ¹æ®æ•°æ®é›†ç±»å‹è®¾ç½®æ­£æ ·æœ¬æƒé‡
    # é¥æ„Ÿæ•°æ®é›†ï¼ˆDSIFN, MassRoadsï¼‰å’Œå·¥ä¸šç¼ºé™·æ•°æ®é›†ï¼ˆViSAï¼‰å‰æ™¯æ¯”ä¾‹å¾ˆå°ï¼Œéœ€è¦å¢åŠ å‰æ™¯æƒ
    # Satelliteæ•°æ®é›†å‰æ™¯æ¯”ä¾‹çº¦50%ï¼Œä¸éœ€è¦æƒ
    if dataset_type in ['dsifn', 'massroads', 'visa']:
        pos_weight = torch.tensor([10.0]).to(device)  # å‰æ™¯æƒé‡10
        print(f"  ä½¿ç”¨æ­£æ ·æœ¬æƒé‡ 10.0 (é€‚ç”¨äº{dataset_type}çš„ç±»åˆ«ä¸å¹³è¡¡)")
    else:
        pos_weight = None
        print(f"  ä½¿ç”¨æ ‡å‡†BCEæŸå¤±ï¼ˆæ— æƒé‡")
    
    if pos_weight is not None:
        bce_loss = nn.BCEWithLogitsLoss(pos_weight=pos_weight)
    else:
        bce_loss = nn.BCEWithLogitsLoss()
    
    def dice_loss(pred, target, smooth=1e-6):
        pred = torch.sigmoid(pred)
        intersection = (pred * target).sum()
        union = pred.sum() + target.sum()
        dice = (2. * intersection + smooth) / (union + smooth)
        return 1 - dice
    
    # è®­ç»ƒå¾ªç¯
    best_iou = 0.0
    patience_counter = 0  # Early stoppingè®¡æ•°
    print(f"\nå¼€å§‹è®­ç»ƒ {k_shot}-shot æ¨¡å‹...")
    print(f"è®­ç»ƒè½®æ•°: {epochs}, æ‰¹æ¬¡å¤§å°: {batch_size}, å­¦ä¹ ç‡ {lr}")
    print(f"Early stopping patience: {early_stopping_patience}")
    
    # åˆ¤æ–­æ˜¯å¦ä½¿ç”¨GLCMï¼ˆæ ¹æ®å‚æ•°å†³å®šï¼‰
    using_glcm = use_glcm
    
    for epoch in range(epochs):
        model.train()
        epoch_loss = 0.0
        epoch_seg_loss = 0.0
        epoch_glcm_loss = 0.0
        
        pbar = tqdm(train_loader, desc=f"Epoch {epoch+1}/{epochs}")
        for images, masks, labels in pbar:  # æ¥æ”¶3ä¸ªå€¼ images, masks, labels
            images = images.to(device)
            masks = masks.to(device).float()  # [B, H, W]
            
            # ç¡®ä¿maskæ˜¯4Dçš„[B, H, W]
            if masks.dim() == 4:  # [B, 1, H, W]
                masks = masks.squeeze(1)  # -> [B, H, W]
            
            # æ‰©å±•ç»´åº¦ç”¨äºlossè®¡ç®— [B, 1, H, W]
            masks_4d = masks.unsqueeze(1)  # [B, 1, H, W]
            
            # å‰å‘ä¼ æ’­ï¼ˆDPTEnhancedè¿”å›3ä¸ªå€¼ï¼šä¸»è¾“å‡ºã€å„å±‚å¼‚å¸¸å›¾ã€èåˆå¼‚å¸¸å›¾
            result = model(images)
            if isinstance(result, tuple) and len(result) == 3:
                outputs, anomaly_maps, anomaly_map_fused = result
            elif isinstance(result, tuple) and len(result) == 2:
                outputs, anomaly_maps = result
                anomaly_map_fused = None
            else:
                outputs = result
                anomaly_maps = None
                anomaly_map_fused = None
            
            # ç¡®ä¿å°ºå¯¸åŒ¹é…
            if outputs.shape[-2:] != masks_4d.shape[-2:]:
                outputs = F.interpolate(outputs, size=masks_4d.shape[-2:],
                                      mode='bilinear', align_corners=False)
            
            # ===== æŸå¤±1: ä¸»åˆ†å‰²æŸå¤±ï¼ˆBCE + Diceï¼‰====
            loss_bce = bce_loss(outputs, masks_4d)
            loss_dice = dice_loss(outputs, masks_4d)
            seg_loss = loss_bce + loss_dice
            
            # ===== æŸå¤±2: GLCMæ ¡å‡†æŸå¤±ï¼ˆå¯é€‰ï¼‰=====
            glcm_loss = 0.0
            if use_glcm and anomaly_maps is not None and len(anomaly_maps) > 0:
                # å¯¹æ¯ä¸€å±‚çš„å¼‚å¸¸å›¾è®¡ç®—ç›‘ç£æŸ
                for anomaly_map in anomaly_maps:
                    if anomaly_map.shape[-2:] != masks_4d.shape[-2:]:
                        anomaly_map = F.interpolate(
                            anomaly_map,
                            size=masks_4d.shape[-2:],
                            mode='bilinear',
                            align_corners=False
                        )
                    # BCEæŸå¤±
                    glcm_bce = F.binary_cross_entropy(anomaly_map, masks_4d)
                    # DiceæŸå¤±
                    intersection = (anomaly_map * masks_4d).sum()
                    union = anomaly_map.sum() + masks_4d.sum()
                    glcm_dice = 1 - (2 * intersection + 1e-6) / (union + 1e-6)
                    glcm_loss += (glcm_bce + glcm_dice)
                
                # å¹³å‡å¤šå±‚æŸå¤±
                glcm_loss = glcm_loss / len(anomaly_maps)
            
            # ===== æŸå¤±3: èåˆGLCMå¼‚å¸¸å›¾çš„æŸå¤±ï¼ˆå¯é€‰ï¼Œä¸ä¸»åˆ†å‰²è¾“å‡ºèåˆï¼‰=====
            glcm_fused_loss = 0.0
            if use_glcm and anomaly_map_fused is not None:
                if anomaly_map_fused.shape[-2:] != masks_4d.shape[-2:]:
                    anomaly_map_fused = F.interpolate(
                        anomaly_map_fused,
                        size=masks_4d.shape[-2:],
                        mode='bilinear',
                        align_corners=False
                    )
                glcm_fused_bce = F.binary_cross_entropy(anomaly_map_fused, masks_4d)
                intersection_fused = (anomaly_map_fused * masks_4d).sum()
                union_fused = anomaly_map_fused.sum() + masks_4d.sum()
                glcm_fused_dice = 1 - (2 * intersection_fused + 1e-6) / (union_fused + 1e-6)
                glcm_fused_loss = glcm_fused_bce + glcm_fused_dice
            
            # ===== æ€»æŸå¤±=====
            # seg_loss: ä¸»åˆ†å‰²æŸå¤±(æƒé‡1.0)
            # glcm_loss: å„å±‚å¼‚å¸¸å›¾è¾…åŠ©æŸå¤±(æƒé‡0.15)
            # glcm_fused_loss: èåˆå¼‚å¸¸å›¾æŸå¤±(æƒé‡0.25ï¼Œå‚è€ƒAD-DINOv3)
            if use_glcm and (glcm_loss > 0 or glcm_fused_loss > 0):
                loss = seg_loss + 0.25 * glcm_fused_loss
            else:
                loss = seg_loss
            
            # åå‘ä¼ æ’­
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
            
            epoch_loss += loss.item()
            epoch_seg_loss += seg_loss.item()
            if use_glcm and glcm_loss > 0:
                epoch_glcm_loss += glcm_loss.item()
            
            # æ›´æ–°è¿›åº¦æ¡ï¼ˆæ˜¾ç¤ºå„é¡¹æŸå¤±
            if use_glcm and (glcm_loss > 0 or glcm_fused_loss > 0):
                pbar.set_postfix({
                    'loss': f'{loss.item():.4f}',
                    'seg': f'{seg_loss.item():.4f}',
                    'glcm_l': f'{glcm_loss.item():.4f}',  # å„å±‚GLCM
                    'glcm_f': f'{glcm_fused_loss.item():.4f}'  # èåˆGLCM
                })
            else:
                pbar.set_postfix({'loss': f'{loss.item():.4f}'})
        
        scheduler.step()
        avg_loss = epoch_loss / len(train_loader)
        avg_seg_loss = epoch_seg_loss / len(train_loader)
        avg_glcm_loss = epoch_glcm_loss / len(train_loader) if using_glcm else 0.0
        
        # éªŒè¯ï¼ˆæ ¹æ®val_interval
        if (epoch + 1) % val_interval == 0:
            model.eval()
            
            # ===== äºŒåˆ†ç±»éªŒè¯ï¼ˆæ‰€æœ‰æ•°æ®é›†ç»Ÿä¸€ä½¿ç”¨ï¼‰====
            val_iou = 0.0
            val_accuracy = 0.0
            val_f1 = 0.0
            val_mae = 0.0
            val_ber = 0.0
            val_dice = 0.0
            val_hd95_list = []
            
            with torch.no_grad():
                    # æ”¶é›†æ‰€æœ‰é¢„æµ‹å’ŒçœŸå®æ ‡ç­¾ç”¨äºæ•´ä½“è®¡ç®—
                    all_preds_list = []
                    all_masks_list = []
                    
                    # ç”¨äºé€æ ·æœ¬è®¡ç®—Dice å’Œ HD95
                    sample_dice_list = []
                    sample_hd95_list = []
                    
                    for batch in tqdm(test_loader, desc="Validation", leave=False):
                        images, masks, _ = batch
                        images = images.to(device)
                        masks = masks.to(device).float()
                        
                        # ç¡®ä¿maskæ˜¯4D
                        if masks.dim() == 4:  # [B, 1, H, W]
                            masks = masks.squeeze(1)  # -> [B, H, W]
                        
                        # å‰å‘ä¼ æ’­ï¼ˆå¤„ç†å…ƒç»„æˆ–å…ƒç»„è¿”å›å€¼ï¼‰
                        result = model(images)
                        if isinstance(result, tuple) and len(result) >= 2:
                            outputs = result[0]  # åªå–ä¸»è¾“
                            # å¯é€‰ï¼šä¹Ÿå¯ä»¥ä½¿ç”¨èåˆçš„anomaly_map
                            # if len(result) == 3 and result[2] is not None:
                            #     outputs = 0.7 * outputs + 0.3 * result[2]  # èåˆç­–ç•¥
                        else:
                            outputs = result
                        
                        if outputs.shape[-2:] != masks.shape[-2:]:
                            outputs = F.interpolate(outputs, size=masks.shape[-2:],
                                                  mode='bilinear', align_corners=False)
                        
                        # æ ¹æ®æ•°æ®é›†ç±»å‹è°ƒæ•´é¢„æµ‹é˜ˆ
                        # DSIFN/MassRoads/ViSAå‰æ™¯ç¨€ç–ç”¨0.3
                        # Satelliteå‰æ™¯æ¯”ä¾‹~23%ï¼Œä½†æ¨¡å‹å€¾å‘è¿‡åº¦é¢„æµ‹ï¼Œç”¨0.6
                        if dataset_type in ['dsifn', 'massroads', 'visa']:
                            threshold = 0.3
                        elif dataset_type == 'satellite':
                            threshold = 0.7
                        else:
                            threshold = 0.5
                        pred = (torch.sigmoid(outputs.squeeze(1)) > threshold).float()  # [B, H, W]
                        
                        # é€æ ·æœ¬è®¡ç®—Dice å’Œ HD95
                        for i in range(pred.shape[0]):
                            pred_np = pred[i].cpu().numpy()
                            mask_np = masks[i].cpu().numpy()
                            
                            # Dice
                            dice = compute_dice_coefficient(pred_np, mask_np)
                            sample_dice_list.append(dice)
                            
                            # HD95
                            hd95 = compute_hd95(pred_np, mask_np)
                            if not np.isinf(hd95):  # åªç»Ÿè®¡æœ‰æ•ˆçš„HD95
                                sample_hd95_list.append(hd95)
                        
                        # å°†æ¯ä¸ªæ ·æœ¬çš„åƒç´ å±•å¹³åæ·»åŠ ï¼ˆå¤„ç†ä¸åŒå°ºå¯¸
                        for i in range(pred.shape[0]):
                            all_preds_list.append(pred[i].flatten())
                            all_masks_list.append(masks[i].flatten())
                    
                    # æ‹¼æ¥æ‰€æœ‰æ ·æœ¬çš„æ‰€æœ‰åƒ
                    all_preds = torch.cat(all_preds_list, dim=0)  # [æ€»åƒç´ æ•°]
                    all_masks = torch.cat(all_masks_list, dim=0)  # [æ€»åƒç´ æ•°]
                
                    # ========== æ•´ä½“åƒç´ è®¡ç®— ==========
                    # è®¡ç®— Micro IoU (æ•´ä½“æ‰€æœ‰åƒç´ ï¼Œä¿ç•™åŸæœ‰é€»è¾‘)
                    intersection = (all_preds * all_masks).sum()
                    union = all_preds.sum() + all_masks.sum() - intersection
                    val_iou_micro = (intersection / (union + 1e-6)).item()
                
                    # è®¡ç®—æ ‡å‡† mIoU (èƒŒæ™¯å’Œå‰æ™¯åˆ†åˆ«è®¡ç®—å†å¹³å‡)
                    # å‰æ™¯ IoU (é¢„æµ‹=1, çœŸå®=1)
                    fg_preds = all_preds  # å‰æ™¯é¢„æµ‹
                    fg_masks = all_masks  # å‰æ™¯çœŸå®
                    fg_intersection = (fg_preds * fg_masks).sum()
                    fg_union = fg_preds.sum() + fg_masks.sum() - fg_intersection
                    val_iou_fg = (fg_intersection / (fg_union + 1e-6)).item()
                
                    # èƒŒæ™¯ IoU (é¢„æµ‹=0,  çœŸå®=0)
                    bg_preds = 1 - all_preds  # èƒŒæ™¯é¢„æµ‹
                    bg_masks = 1 - all_masks  # èƒŒæ™¯çœŸå®
                    bg_intersection = (bg_preds * bg_masks).sum()
                    bg_union = bg_preds.sum() + bg_masks.sum() - bg_intersection
                    val_iou_bg = (bg_intersection / (bg_union + 1e-6)).item()
                
                    # mIoU = ä¸¤ç±»IoUçš„å¹³
                    val_miou = (val_iou_bg + val_iou_fg) / 2.0
                
                    # è®¡ç®—Accuracy
                    correct = (all_preds == all_masks).sum()
                    total = all_masks.numel()
                    val_accuracy = (correct / total).item()
                
                    # è®¡ç®—F1-Score
                    tp = (all_preds * all_masks).sum()
                    fp = (all_preds * (1 - all_masks)).sum()
                    fn = ((1 - all_preds) * all_masks).sum()
                    tn = ((1 - all_preds) * (1 - all_masks)).sum()
                
                    precision = tp / (tp + fp + 1e-6)
                    recall = tp / (tp + fn + 1e-6)
                    val_f1 = (2 * precision * recall / (precision + recall + 1e-6)).item()
                
                    # è®¡ç®—MAE
                    val_mae = torch.abs(all_preds - all_masks).mean().item()
                
                    # è®¡ç®—BER
                    fpr = fp / (fp + tn + 1e-6)  # False Positive Rate
                    fnr = fn / (fn + tp + 1e-6)  # False Negative Rate
                    val_ber = (0.5 * (fpr + fnr)).item()
                
                    # ========== é€æ ·æœ¬å¹³å‡è®¡ç®—==========
                    # Dice (å¹³å‡)
                    val_dice = np.mean(sample_dice_list) if sample_dice_list else 0.0
                
                    # HD95 (å¹³å‡)
                    val_hd95 = np.mean(sample_hd95_list) if sample_hd95_list else np.inf
            
            print(f"\nEpoch [{epoch+1}/{epochs}]")
            if using_glcm and avg_glcm_loss > 0:
                print(f"  Loss: {avg_loss:.4f} (Seg: {avg_seg_loss:.4f}, GLCM: {avg_glcm_loss:.4f})")
            else:
                print(f"  Loss: {avg_loss:.4f}")
            print(f"  Val mIoU (æ ‡å‡†): {val_miou:.4f}  [èƒŒæ™¯: {val_iou_bg:.4f}, å‰æ™¯: {val_iou_fg:.4f}]")
            print(f"  Val IoU (micro): {val_iou_micro:.4f}")
            print(f"  Val Dice: {val_dice:.4f}")
            print(f"  Val HD95: {val_hd95:.2f}" if not np.isinf(val_hd95) else f"  Val HD95: inf")
            print(f"  Val Accuracy: {val_accuracy:.4f}")
            print(f"  Val F1-Score: {val_f1:.4f}")
            print(f"  Val MAE: {val_mae:.4f}")
            print(f"  Val BER: {val_ber:.4f}")
            
            # ========== å¯è§†åŒ–éªŒè¯ç»“æœ(æ¯ä¸ªepochæˆ–æœ€åä¸€ä¸ªepoch) ==========
            if (epoch + 1) % 5 == 0 or epoch == epochs - 1:
                print(f"\nğŸ“Š ç”ŸæˆéªŒè¯å¯è§†åŒ–(Epoch {epoch+1})...")
                vis_dir = os.path.join(output_dir, f'visualizations/epoch_{epoch+1}')
                os.makedirs(vis_dir, exist_ok=True)
                
                # éšæœºé€‰æ‹©5ä¸ªéªŒè¯æ ·æœ¬è¿›è¡Œå¯è§†åŒ–
                num_vis_samples = min(5, len(test_dataset))
                vis_indices = np.random.choice(len(test_dataset), num_vis_samples, replace=False)
                
                model.eval()
                with torch.no_grad():
                    for vis_idx, sample_idx in enumerate(vis_indices):
                        # è·å–æ ·æœ¬
                        if isinstance(test_dataset, torch.utils.data.Subset):
                            image, mask, info = test_dataset.dataset[test_dataset.indices[sample_idx]]
                        else:
                            image, mask, info = test_dataset[sample_idx]
                        
                        image_input = image.unsqueeze(0).to(device)
                        
                        # é¢„æµ‹
                        result = model(image_input)
                        if isinstance(result, tuple):
                            output = result[0]
                        else:
                            output = result
                        
                        if output.shape[-2:] != mask.shape[-2:]:
                            output = F.interpolate(output, size=mask.shape[-2:],
                                                  mode='bilinear', align_corners=False)
                        
                        # ä½¿ç”¨ä¸éªŒè¯ç›¸åŒçš„é˜ˆ
                        if dataset_type in ['dsifn', 'massroads', 'visa']:
                            vis_threshold = 0.3
                        elif dataset_type == 'satellite':
                            vis_threshold = 0.9
                        else:
                            vis_threshold = 0.5
                        
                        pred_prob = torch.sigmoid(output).squeeze().cpu().numpy()
                        pred_mask = (pred_prob > vis_threshold).astype(np.uint8)
                        gt_mask = mask.cpu().numpy().astype(np.uint8)
                        
                        # ç¡®ä¿gt_maskæ˜¯2Dçš„ [H, W]
                        if gt_mask.ndim == 3:
                            gt_mask = gt_mask.squeeze(0)
                        
                        # åå½’ä¸€åŒ–å›¾
                        image_np = image.cpu().numpy().transpose(1, 2, 0)
                        mean = np.array([0.485, 0.456, 0.406])
                        std = np.array([0.229, 0.224, 0.225])
                        image_np = image_np * std + mean
                        image_np = np.clip(image_np, 0, 1)
                        
                        # è®¡ç®—IoU
                        intersection = np.logical_and(pred_mask, gt_mask).sum()
                        union = np.logical_or(pred_mask, gt_mask).sum()
                        iou = intersection / (union + 1e-6)
                        
                        # å¤„ç†æ ·æœ¬ä¿¡æ¯
                        sample_info = str(sample_idx)
                        full_path = ""
                        if isinstance(info, str):
                            full_path = info
                            # å¦‚æœæ˜¯è·¯å¾„ï¼Œå°è¯•æå–æ–‡ä»¶
                            if '/' in info or '\\' in info:
                                sample_info = os.path.basename(info)
                            else:
                                sample_info = info
                        elif isinstance(info, dict):
                            # å¤„ç†FolderDatasetè¿”å›çš„å­—å…¸ä¿¡
                            if 'img_path' in info:
                                full_path = info['img_path']
                                sample_info = os.path.basename(full_path)
                            elif 'id' in info:
                                sample_info = info['id']

                        # åˆ›å»ºå¯è§†åŒ–(2x3å¸ƒå±€)
                        import matplotlib.pyplot as plt
                        fig, axes = plt.subplots(2, 3, figsize=(15, 10))
                        
                        # Row 1
                        axes[0, 0].imshow(image_np)
                        axes[0, 0].set_title(f'Original Image\n{sample_info}', fontsize=14, fontweight='bold')
                        axes[0, 0].axis('off')
                        
                        axes[0, 1].imshow(gt_mask, cmap='gray')
                        axes[0, 1].set_title(f'Ground Truth\nForeground: {gt_mask.sum():,} ({100*gt_mask.mean():.1f}%)', 
                                           fontsize=14, fontweight='bold')
                        axes[0, 1].axis('off')
                        
                        axes[0, 2].imshow(pred_mask, cmap='gray')
                        axes[0, 2].set_title(f'Prediction\nForeground: {pred_mask.sum():,} ({100*pred_mask.mean():.1f}%)', 
                                           fontsize=14, fontweight='bold')
                        axes[0, 2].axis('off')
                        
                        # Row 2
                        axes[1, 0].imshow(pred_prob, cmap='jet', vmin=0, vmax=1)
                        axes[1, 0].set_title('Probability Map', fontsize=14, fontweight='bold')
                        axes[1, 0].axis('off')
                        plt.colorbar(axes[1, 0].images[0], ax=axes[1, 0], fraction=0.046, pad=0.04)
                        
                        # Overlay
                        overlay = image_np.copy()
                        tp_mask = np.logical_and(pred_mask == 1, gt_mask == 1)
                        fp_mask = np.logical_and(pred_mask == 1, gt_mask == 0)
                        fn_mask = np.logical_and(pred_mask == 0, gt_mask == 1)
                        
                        overlay[tp_mask] = [0, 1, 0]  # Green
                        overlay[fp_mask] = [1, 0, 0]  # Red
                        overlay[fn_mask] = [0, 0, 1]  # Blue
                        
                        axes[1, 1].imshow(overlay)
                        axes[1, 1].set_title('Overlay\nGreen=TP, Red=FP, Blue=FN', fontsize=14, fontweight='bold')
                        axes[1, 1].axis('off')
                        
                        # Metrics display
                        axes[1, 2].axis('off')
                        stats_text = f"""
Epoch {epoch+1} Validation

IoU: {iou:.4f}
Threshold: {vis_threshold}

Probability Stats:
  Min: {pred_prob.min():.4f}
  Max: {pred_prob.max():.4f}
  Mean: {pred_prob.mean():.4f}
  Std: {pred_prob.std():.4f}

Sample: {full_path if full_path else sample_idx}
                        """
                        axes[1, 2].text(0.1, 0.5, stats_text, fontsize=11, 
                                       verticalalignment='center',
                                       bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.3))
                        
                        fig.suptitle(f'Epoch {epoch+1} - éªŒè¯æ ·æœ¬ {vis_idx+1}/{num_vis_samples}', 
                                    fontsize=16, fontweight='bold')
                        plt.tight_layout()
                        
                        save_path = os.path.join(vis_dir, f'val_sample_{sample_idx:04d}.png')
                        plt.savefig(save_path, dpi=100, bbox_inches='tight')
                        plt.close()
                
                print(f"âœ… å¯è§†åŒ–å·²ä¿å­˜åˆ° {vis_dir}")
            
            # ä¿å­˜æœ€ä½³æ¨¡å‹(ä½¿ç”¨ mIoU ä½œä¸ºé€‰æ‹©æ ‡å‡†)
            if val_miou > best_iou:
                best_iou = val_miou
                patience_counter = 0  # é‡ç½®è®¡æ•°
                model_path = os.path.join(output_dir, f'best_model_{k_shot}shot.pth')
                torch.save({
                    'epoch': epoch,
                    'model_state_dict': model.state_dict(),
                    'optimizer_state_dict': optimizer.state_dict(),
                    'val_miou': val_miou,
                    'val_iou_micro': val_iou_micro,
                    'val_iou_bg': val_iou_bg,
                    'val_iou_fg': val_iou_fg,
                    'val_dice': val_dice,
                    'val_hd95': val_hd95 if not np.isinf(val_hd95) else -1,
                    'val_accuracy': val_accuracy,
                    'val_f1': val_f1,
                    'val_mae': val_mae,
                    'val_ber': val_ber,
                    'k_shot': k_shot,
                    'use_hypergraph': use_hypergraph,
                    'use_layers': use_layers,
                    'dataset_type': dataset_type,
                }, model_path)
                print(f"âœ… ä¿å­˜æœ€ä½³æ¨¡å‹(Val mIoU: {best_iou:.4f}, Dice: {val_dice:.4f}, HD95: {val_hd95:.2f})" 
                      if not np.isinf(val_hd95) else 
                      f"âœ… ä¿å­˜æœ€ä½³æ¨¡å‹(Val mIoU: {best_iou:.4f}, Dice: {val_dice:.4f})")
            else:
                patience_counter += 1
                print(f"Val mIoUæœªæå‡({patience_counter}/{early_stopping_patience})")
                
                # Early stoppingæ£€
                if patience_counter >= early_stopping_patience:
                    print(f"\nâš ï¸ Early stopping triggered at epoch {epoch+1}")
                    print(f"Best Val mIoU: {best_iou:.4f} (stopped after {early_stopping_patience} epochs without improvement)")
                    break
    
    print(f"\nè®­ç»ƒå®Œæˆ! æœ€ä½³Val mIoU: {best_iou:.4f}")

    # å¯è§†åŒ–ç‰¹å®šå›¾åƒ(Satellite 3_1.tif)
    if dataset_type == 'satellite':
        best_model_path = os.path.join(output_dir, f'best_model_{k_shot}shot.pth')
        if os.path.exists(best_model_path):
            # Load best weights
            checkpoint = torch.load(best_model_path)
            model.load_state_dict(checkpoint['model_state_dict'])
            
            vis_save_path = os.path.join(output_dir, f'vis_3_1_{k_shot}shot.png')
            # Try to find in test dataset first, then train dataset
            found = visualize_specific_satellite_image(model, test_dataset, '3_1.tif', vis_save_path, device)
            if not found:
                print("  åœ¨æµ‹è¯•é›†ä¸­æœªæ‰¾åˆ°ï¼Œå°è¯•è®­ç»ƒé›†...")
                visualize_specific_satellite_image(model, train_dataset, '3_1.tif', vis_save_path, device)

    return os.path.join(output_dir, f'best_model_{k_shot}shot.pth')


def main():
    parser = argparse.ArgumentParser(description='Few-shot Learning (TN3K/MVTec/ViSA)')
    
    # æ•°æ®é›†ç›¸å…³å‚
    parser.add_argument('--dataset_type', type=str, default='tn3k',
                      choices=['tn3k', 'mvtec', 'visa', 'dsifn', 'massroads', 'satellite'],
                      help='æ•°æ®é›†ç±»å‹ tn3k, mvtec, visa, dsifn (é¥æ„Ÿå˜åŒ–æ£€æµ‹, massroads (é¥æ„Ÿé“è·¯), satellite (é¥æ„Ÿåˆ†å‰²)')
    parser.add_argument('--data_dir', type=str, default='./segdata/tn3k',
                      help='æ•°æ®é›†è·¯')
    parser.add_argument('--mvtec_category', type=str, default=None,
                      choices=['all', 'bottle', 'cable', 'capsule', 'carpet', 'grid',
                              'hazelnut', 'leather', 'metal_nut', 'pill', 'screw',
                              'tile', 'toothbrush', 'transistor', 'wood', 'zipper'],
                      help='MVTecç±»åˆ«: all=æ‰€æœ‰ 5ä¸ªç±»å‹ æˆ–æŒ‡å®šå•ä¸ªç±»å‹(ä»…å½“dataset_type=mvtecæ—¶éœ€è¦')
    parser.add_argument('--visa_category', type=str, default=None,
                      help='ViSAç±»åˆ« (å¦‚ candle, capsulesç­‰) (ä»…å½“dataset_type=visaæ—¶éœ€è¦')
    parser.add_argument('--visa_csv', type=str, default='split_csv/2cls_fewshot.csv',
                      help='ViSAçš„CSVæ–‡ä»¶è·¯å¾„')
    parser.add_argument('--include_normal', action='store_true',
                      help='ViSAæ˜¯å¦åŒ…å«æ­£å¸¸æ ·æœ¬ï¼ˆä»…å½“dataset_type=visaæ—¶ï¼‰')
    parser.add_argument('--output_dir', type=str, default='./runs/tn3k_fewshot',
                      help='è¾“å‡ºç›®å½•')
    parser.add_argument('--val_interval', type=int, default=10,
                      help='éªŒè¯é—´éš”ï¼ˆæ¯Nä¸ªepochéªŒè¯ä¸€æ¬¡ï¼‰')
    
    # è®­ç»ƒå‚æ•°
    parser.add_argument('--k_shots', type=int, nargs='+', default=[5, 10, 20],
                      help='Few-shotæ•°é‡')
    parser.add_argument('--epochs', type=int, default=50,
                      help='è®­ç»ƒè½®æ•°')
    parser.add_argument('--batch_size', type=int, default=4,
                      help='æ‰¹æ¬¡å¤§å°')
    parser.add_argument('--lr', type=float, default=1e-4,
                      help='å­¦ä¹ ç‡')
    parser.add_argument('--early_stopping_patience', type=int, default=10,
                      help='Early stoppingè€å¿ƒå€¼ï¼ˆè¿ç»­å¤šå°‘ä¸ªepochä¸æå‡åˆ™åœæ­¢')
    parser.add_argument('--augment_factor', type=int, default=10,
                      help='æ•°æ®å¢å¼ºå€æ•°')
    parser.add_argument('--sampling_strategy', type=str, default='top',
                      choices=['top', 'diverse'],
                      help='é‡‡æ ·ç­–ç•¥: top-å‰æ™¯æœ€å¤š diverse-å‡åŒ€åˆ†å¸ƒ')
    
    # æ¨¡å‹å¢å¼ºæ¨¡å—å‚æ•°
    parser.add_argument('--use_internal_adapter', action='store_true',
                      help='åœ¨DINOv3å†…éƒ¨æ³¨å…¥é€‚é…å™¨ï¼ˆä½¿å†»ç»“éª¨å¹²ç½‘ç»œé€‚åº”æ•°æ®é›†ï¼‰')
    parser.add_argument('--use_glcm', action='store_true',
                      help='ä½¿ç”¨GLCMå…¨å±€-å±€éƒ¨æ ¡å‡†æ¨¡å—')
    parser.add_argument('--use_hypergraph', action='store_true',
                      help='ä½¿ç”¨è¶…å›¾GCNæ¨¡å—')
    parser.add_argument('--use_layers', type=str, default='6_9',
                      choices=['all', '6_9'],
                      help='DINOv2ç‰¹å¾å±‚: all-4å±‚, 6_9-2å±‚')
    parser.add_argument('--device', type=str, default='cuda',
                      help='è®¾å¤‡')
    parser.add_argument('--seed', type=int, default=42,
                      help='éšæœºç§å­ï¼ˆç¡®ä¿å¯å¤ç°')
    
    args = parser.parse_args()
    
    # æ•°æ®é›†éªŒ
    if args.dataset_type == 'mvtec':
        if args.mvtec_category is None:
            # é»˜è®¤è®­ç»ƒæ‰€æœ‰ç±»
            print("æœªæŒ‡å®šMVTecç±»åˆ«ï¼Œå°†è®­ç»ƒæ‰€æœ‰ 5ä¸ªç±»åˆ«")
            args.mvtec_category = 'all'
    elif args.dataset_type == 'visa':
        if args.visa_category is None:
            raise ValueError("ä½¿ç”¨ViSAæ•°æ®é›†æ—¶å¿…é¡»æŒ‡å®š --visa_category")
    
    print("="*80)
    print(f"{args.dataset_type.upper()} Few-shot Learning å®éªŒ")
    print("="*80)
    print(f"æ•°æ®é›†ç±»å‹ {args.dataset_type}")
    if args.dataset_type == 'mvtec':
        if args.mvtec_category == 'all':
            print(f"MVTecç±»åˆ«: æ‰€æœ‰ 5ä¸ªç±»åˆ«ï¼ˆè”åˆè®­ç»ƒï¼‰")
        else:
            print(f"MVTecç±»åˆ«: {args.mvtec_category}")
    elif args.dataset_type == 'visa':
        print(f"ViSAç±»åˆ«: {args.visa_category}")
        print(f"CSVæ–‡ä»¶: {args.visa_csv}")
        print(f"åŒ…å«æ­£å¸¸æ ·æœ¬: {args.include_normal}")
    print(f"æ•°æ®ç›®å½•: {args.data_dir}")
    print(f"è¾“å‡ºç›®å½•: {args.output_dir}")
    print(f"K-shotè®¾ç½®: {args.k_shots}")
    print(f"è®­ç»ƒè½®æ•°: {args.epochs}")
    print(f"éªŒè¯é—´éš”: {args.val_interval}")
    print(f"å¢å¼ºå€æ•°: {args.augment_factor}")
    print(f"é‡‡æ ·ç­–ç•¥: {args.sampling_strategy}")
    print(f"ä½¿ç”¨Internal Adapter: {args.use_internal_adapter}")
    print(f"ä½¿ç”¨GLCM: {args.use_glcm}")
    print(f"ä½¿ç”¨è¶…å›¾GCN: {args.use_hypergraph}")
    print(f"ç‰¹å¾å±‚é…ç½® {args.use_layers}")
    print(f"Early stopping: {args.early_stopping_patience}")
    print(f"éšæœºç§å­: {args.seed}")
    print("="*80)
    
    # å¯¹æ¯ä¸ªk_shotè¿›è¡Œå®éªŒ
    results = {}
    for k_shot in args.k_shots:
        print(f"\n{'='*80}")
        print(f"å¼€å§‹{k_shot}-shot å®éªŒ")
        print(f"{'='*80}")
        
        output_dir = os.path.join(args.output_dir, f'{k_shot}shot')
        
        model_path = train_few_shot_tn3k(
            data_dir=args.data_dir,
            output_dir=output_dir,
            k_shot=k_shot,
            epochs=args.epochs,
            batch_size=args.batch_size,
            lr=args.lr,
            augment_factor=args.augment_factor,
            sampling_strategy=args.sampling_strategy,
            use_internal_adapter=args.use_internal_adapter,  # æ–°å¢
            use_glcm=args.use_glcm,
            use_hypergraph=args.use_hypergraph,
            use_layers=args.use_layers,
            device=args.device,
            early_stopping_patience=args.early_stopping_patience,
            seed=args.seed,
            dataset_type=args.dataset_type,
            mvtec_category=args.mvtec_category,
            visa_category=args.visa_category,
            visa_csv=args.visa_csv,
            include_normal=args.include_normal,
            val_interval=args.val_interval
        )
        
        results[k_shot] = model_path
    
    # æ‰“å°ç»“æœæ€»ç»“
    print("\n" + "="*80)
    print("å®éªŒç»“æœæ€»ç»“")
    print("="*80)
    for k_shot, model_path in results.items():
        print(f"{k_shot}-shot: {model_path}")
    print("="*80)


if __name__ == '__main__':
    main()
